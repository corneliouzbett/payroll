SET SQL_MODE = "NO_AUTO_VALUE_ON_ZERO";
SET time_zone = "+00:00";

/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!40101 SET NAMES utf8 */;

CREATE DATABASE IF NOT EXISTS `payroll` DEFAULT CHARACTER SET utf8 COLLATE utf8_general_ci;
USE `payroll`;

DELIMITER $$
DROP PROCEDURE IF EXISTS `getPayroll`$$
CREATE DEFINER=`root`@`localhost` PROCEDURE `getPayroll`(IN `year` INT, IN `month` INT)
    MODIFIES SQL DATA
BEGIN
	DECLARE myemplyeee_id INT;
	DECLARE myid INT;
	DECLARE myhour INT;
	DECLARE mysalary DOUBLE;
	DECLARE myemployeeType_id INT;
	DECLARE mysalaryPerHour DOUBLE;
	DECLARE done int default -1;

	DECLARE cur CURSOR FOR
	SELECT payroll_employee.id,SUM(hour) as hour
	FROM payroll_employee,payroll_attendrecord 
	WHERE payroll_employee.id=payroll_attendrecord.employee_id and YEAR(date)=year and MONTH(date)=month
	GROUP BY(payroll_employee.id);

	DECLARE continue handler for not found set done=1;

	open cur;
	myLoop: LOOP  
		fetch cur into myid,myhour;
		if done = 1 then   
		leave myLoop;  
		end if;  

		if myhour is null THEN
		set myhour = 0;
		end if;

		SELECT type_id INTO myemployeeType_id
		FROM payroll_employee
		where id=myid;

		SELECT salaryPerHour INTO mysalaryPerHour
		from payroll_employeeType
		where id=myemployeeType_id;	

		set mysalary = mysalaryPerHour * myhour;
		INSERT INTO `payroll_payroll`(`id`, `employee_id`, `employeeType_id`, `month`, `year`, `hour`, `salary`)
		VALUES (null,myid,myemployeeType_id,month,year,myhour,mysalary);

	end loop myLoop; 
	close cur;  
END$$

DELIMITER ;

DROP TABLE IF EXISTS `auth_group`;
CREATE TABLE IF NOT EXISTS `auth_group` (
  `id` int(11) NOT NULL,
  `name` varchar(80) NOT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

DROP TABLE IF EXISTS `auth_group_permissions`;
CREATE TABLE IF NOT EXISTS `auth_group_permissions` (
  `id` int(11) NOT NULL,
  `group_id` int(11) NOT NULL,
  `permission_id` int(11) NOT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

DROP TABLE IF EXISTS `auth_permission`;
CREATE TABLE IF NOT EXISTS `auth_permission` (
  `id` int(11) NOT NULL,
  `name` varchar(255) NOT NULL,
  `content_type_id` int(11) NOT NULL,
  `codename` varchar(100) NOT NULL
) ENGINE=InnoDB AUTO_INCREMENT=43 DEFAULT CHARSET=utf8;

INSERT INTO `auth_permission` (`id`, `name`, `content_type_id`, `codename`) VALUES
(1, 'Can add log entry', 1, 'add_logentry'),
(2, 'Can change log entry', 1, 'change_logentry'),
(3, 'Can delete log entry', 1, 'delete_logentry'),
(4, 'Can add permission', 2, 'add_permission'),
(5, 'Can change permission', 2, 'change_permission'),
(6, 'Can delete permission', 2, 'delete_permission'),
(7, 'Can add group', 3, 'add_group'),
(8, 'Can change group', 3, 'change_group'),
(9, 'Can delete group', 3, 'delete_group'),
(10, 'Can add user', 4, 'add_user'),
(11, 'Can change user', 4, 'change_user'),
(12, 'Can delete user', 4, 'delete_user'),
(13, 'Can add content type', 5, 'add_contenttype'),
(14, 'Can change content type', 5, 'change_contenttype'),
(15, 'Can delete content type', 5, 'delete_contenttype'),
(16, 'Can add session', 6, 'add_session'),
(17, 'Can change session', 6, 'change_session'),
(18, 'Can delete session', 6, 'delete_session'),
(19, 'Can add employee type', 7, 'add_employeetype'),
(20, 'Can change employee type', 7, 'change_employeetype'),
(21, 'Can delete employee type', 7, 'delete_employeetype'),
(22, 'Can add department', 8, 'add_department'),
(23, 'Can change department', 8, 'change_department'),
(24, 'Can delete department', 8, 'delete_department'),
(25, 'Can add employee', 9, 'add_employee'),
(26, 'Can change employee', 9, 'change_employee'),
(27, 'Can delete employee', 9, 'delete_employee'),
(28, 'Can add attend record', 10, 'add_attendrecord'),
(29, 'Can change attend record', 10, 'change_attendrecord'),
(30, 'Can delete attend record', 10, 'delete_attendrecord'),
(31, 'Can add notice', 11, 'add_notice'),
(32, 'Can change notice', 11, 'change_notice'),
(33, 'Can delete notice', 11, 'delete_notice'),
(34, 'Can add post', 12, 'add_post'),
(35, 'Can change post', 12, 'change_post'),
(36, 'Can delete post', 12, 'delete_post'),
(37, 'Can add comment', 13, 'add_comment'),
(38, 'Can change comment', 13, 'change_comment'),
(39, 'Can delete comment', 13, 'delete_comment'),
(40, 'Can add payroll', 14, 'add_payroll'),
(41, 'Can change payroll', 14, 'change_payroll'),
(42, 'Can delete payroll', 14, 'delete_payroll');

DROP TABLE IF EXISTS `auth_user`;
CREATE TABLE IF NOT EXISTS `auth_user` (
  `id` int(11) NOT NULL,
  `password` varchar(128) NOT NULL,
  `last_login` datetime(6) DEFAULT NULL,
  `is_superuser` tinyint(1) NOT NULL,
  `username` varchar(30) NOT NULL,
  `first_name` varchar(30) NOT NULL,
  `last_name` varchar(30) NOT NULL,
  `email` varchar(254) NOT NULL,
  `is_staff` tinyint(1) NOT NULL,
  `is_active` tinyint(1) NOT NULL,
  `date_joined` datetime(6) NOT NULL
) ENGINE=InnoDB AUTO_INCREMENT=12 DEFAULT CHARSET=utf8;

INSERT INTO `auth_user` (`id`, `password`, `last_login`, `is_superuser`, `username`, `first_name`, `last_name`, `email`, `is_staff`, `is_active`, `date_joined`) VALUES
(1, 'pbkdf2_sha256$24000$T1OIkaC9o292$lFSiBL3YCoOqMef5Cgh3dsQOUN9J9nwU0pDeMyc68RQ=', '2016-06-06 09:59:02.622871', 1, 'admin', '', '', 'fangpin1993@hotmail.com', 1, 1, '2016-05-31 08:00:14.324246'),
(2, 'pbkdf2_sha256$24000$qgQtSa2zzKGT$NeJhw6PW2CV1UUr+FmCn4poeh/j20oQdmfb0AogvlVw=', '2016-06-06 10:04:02.949430', 0, 'fangpin', '', '', '', 0, 1, '2016-05-31 08:01:48.521397'),
(3, 'pbkdf2_sha256$24000$1d94ULiwaxLL$Mf34Yr158w4mGf3rebQlj8dbU5+PBQHvlw7XDzN8pwI=', '2016-06-05 07:03:22.245109', 0, 'zhangsan', '', '', '', 0, 1, '2016-06-04 11:58:21.166036'),
(4, 'pbkdf2_sha256$24000$v914GbsX1CeD$qsbtjZpXNq4XndaMS+Ma6fjgx1Z20+zQTBgWhWrRypc=', '2016-06-05 08:09:45.919629', 0, 'lisi', '', '', '', 0, 1, '2016-06-05 06:39:32.435555'),
(5, 'pbkdf2_sha256$24000$nX7MbcWWUYEZ$zlVIMVuuTApeQtNK/VSEbhKN/zOtJsjdEbHTiaaPZO0=', '2016-06-05 07:10:38.549398', 0, 'wanger', '', '', '', 0, 1, '2016-06-05 06:41:04.382689'),
(6, 'pbkdf2_sha256$24000$YjkUZLKv6Fqg$RGswx64enxqv2kZjY8Gw5bdOJ/EfbRzdCnuiCJcRqmk=', '2016-06-05 07:13:17.772866', 0, 'qianwu', '', '', '', 0, 1, '2016-06-05 06:42:24.518715'),
(7, 'pbkdf2_sha256$24000$XqpDyl8mRnyw$DLYjs8z/mt+dYn7Al8q40CagMg9SyY1kE1/pPGlPQO4=', '2016-06-05 07:15:11.871216', 0, 'zhengliu', '', '', '', 0, 1, '2016-06-05 06:44:05.839317'),
(8, 'pbkdf2_sha256$24000$CFm3idvKN7po$9Ej7D58H+kWvPTKlFhHEExqvBw5f2SC5wahJDr+Yokc=', '2016-06-05 07:18:07.238222', 0, 'sunqi', '', '', '', 0, 1, '2016-06-05 06:45:09.727105'),
(9, 'pbkdf2_sha256$24000$FOtJls2Rvg2G$y1qBZiEcAtL7Q1IrjQhtdAXvEWI1ohbvonOwTKqhWJs=', NULL, 0, 'wangjiu', '', '', '', 0, 1, '2016-06-05 06:46:21.861797'),
(10, 'pbkdf2_sha256$24000$VUUMiHwQRWrW$KFGcXt0EJ2gWLDECDCoen7O0bFICm+4X968jNpBAVzk=', NULL, 0, 'mashi', '', '', '', 0, 1, '2016-06-05 06:47:19.762142'),
(11, 'pbkdf2_sha256$24000$FLFuVd0Frslf$T3H6il2ujdz1rHwLTMU8rQTy7c2EWhRh6bo0xaFNHK0=', NULL, 0, 'tangba', '', '', '', 0, 1, '2016-06-05 06:49:12.783373');

DROP TABLE IF EXISTS `auth_user_groups`;
CREATE TABLE IF NOT EXISTS `auth_user_groups` (
  `id` int(11) NOT NULL,
  `user_id` int(11) NOT NULL,
  `group_id` int(11) NOT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

DROP TABLE IF EXISTS `auth_user_user_permissions`;
CREATE TABLE IF NOT EXISTS `auth_user_user_permissions` (
  `id` int(11) NOT NULL,
  `user_id` int(11) NOT NULL,
  `permission_id` int(11) NOT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

DROP TABLE IF EXISTS `django_admin_log`;
CREATE TABLE IF NOT EXISTS `django_admin_log` (
  `id` int(11) NOT NULL,
  `action_time` datetime(6) NOT NULL,
  `object_id` longtext,
  `object_repr` varchar(200) NOT NULL,
  `action_flag` smallint(5) unsigned NOT NULL,
  `change_message` longtext NOT NULL,
  `content_type_id` int(11) DEFAULT NULL,
  `user_id` int(11) NOT NULL
) ENGINE=InnoDB AUTO_INCREMENT=63 DEFAULT CHARSET=utf8;

INSERT INTO `django_admin_log` (`id`, `action_time`, `object_id`, `object_repr`, `action_flag`, `change_message`, `content_type_id`, `user_id`) VALUES
(1, '2016-05-31 08:01:48.639680', '2', 'fangpin', 1, 'Added.', 4, 1),
(2, '2016-05-31 08:02:09.430516', '1', '实习', 1, 'Added.', 7, 1),
(3, '2016-05-31 08:02:23.418939', '1', '研发部', 1, 'Added.', 8, 1),
(4, '2016-05-31 08:02:50.628485', '1', '方品', 1, 'Added.', 9, 1),
(5, '2016-05-31 08:14:03.610340', '1', '方品 2016-05-31', 1, 'Added.', 10, 1),
(6, '2016-05-31 08:17:26.897229', '2', '方品 2016-05-30', 1, 'Added.', 10, 1),
(7, '2016-05-31 08:18:14.584978', '2', '方品 2016-05-30', 2, '已修改 hour 。', 10, 1),
(8, '2016-05-31 08:18:30.635630', '2', '方品 2016-05-30', 2, '没有字段被修改。', 10, 1),
(9, '2016-05-31 08:18:55.746510', '1', '方品 2016-05-31', 2, '已修改 start 和 end 。', 10, 1),
(10, '2016-05-31 08:19:07.381207', '1', '方品 2016-05-31', 2, '没有字段被修改。', 10, 1),
(11, '2016-05-31 08:19:35.374655', '1', '方品 2016-05-31', 2, '没有字段被修改。', 10, 1),
(12, '2016-05-31 08:20:13.027383', '1', '方品 2016-05-31', 2, '没有字段被修改。', 10, 1),
(13, '2016-05-31 08:20:17.039381', '1', '方品 2016-05-31', 2, '没有字段被修改。', 10, 1),
(14, '2016-05-31 08:20:20.161631', '2', '方品 2016-05-30', 2, '没有字段被修改。', 10, 1),
(15, '2016-05-31 08:20:25.596724', '1', '方品 2016-05-31', 2, '没有字段被修改。', 10, 1),
(16, '2016-05-31 08:21:11.137121', '2', '方品 2016-05-30', 2, '没有字段被修改。', 10, 1),
(17, '2016-05-31 08:21:18.219864', '1', '方品 2016-05-31', 2, '没有字段被修改。', 10, 1),
(18, '2016-05-31 08:21:52.533461', '1', '方品 2016-05-31', 2, '没有字段被修改。', 10, 1),
(19, '2016-05-31 08:23:55.599336', '1', '方品 2016-05-31', 2, '已修改 hour 。', 10, 1),
(20, '2016-06-04 11:58:21.297120', '3', 'zhangsan', 1, 'Added.', 4, 1),
(21, '2016-06-04 11:58:58.485582', '2', '高级hr', 1, 'Added.', 7, 1),
(22, '2016-06-04 11:59:21.605541', '2', '人事部', 1, 'Added.', 8, 1),
(23, '2016-06-04 11:59:44.514387', '2', '张三', 1, 'Added.', 9, 1),
(24, '2016-06-05 06:39:32.595172', '4', 'lisi', 1, 'Added.', 4, 1),
(25, '2016-06-05 06:40:01.328711', '3', '高级会计', 1, 'Added.', 7, 1),
(26, '2016-06-05 06:40:26.361196', '3', '财务部', 1, 'Added.', 8, 1),
(27, '2016-06-05 06:40:41.298867', '3', '李四', 1, 'Added.', 9, 1),
(28, '2016-06-05 06:41:04.518088', '5', 'wanger', 1, 'Added.', 4, 1),
(29, '2016-06-05 06:41:30.101863', '4', '高级软件工程师', 1, 'Added.', 7, 1),
(30, '2016-06-05 06:42:00.106104', '4', '王二', 1, 'Added.', 9, 1),
(31, '2016-06-05 06:42:24.642600', '6', 'qianwu', 1, 'Added.', 4, 1),
(32, '2016-06-05 06:43:17.452781', '5', '部门经理', 1, 'Added.', 7, 1),
(33, '2016-06-05 06:43:49.344187', '5', '钱五', 1, 'Added.', 9, 1),
(34, '2016-06-05 06:44:05.964451', '7', 'zhengliu', 1, 'Added.', 4, 1),
(35, '2016-06-05 06:44:49.103128', '6', '郑六', 1, 'Added.', 9, 1),
(36, '2016-06-05 06:45:09.857482', '8', 'sunqi', 1, 'Added.', 4, 1),
(37, '2016-06-05 06:46:01.374437', '7', '孙七', 1, 'Added.', 9, 1),
(38, '2016-06-05 06:46:21.980380', '9', 'wangjiu', 1, 'Added.', 4, 1),
(39, '2016-06-05 06:47:02.455122', '8', '王九', 1, 'Added.', 9, 1),
(40, '2016-06-05 06:47:19.886745', '10', 'mashi', 1, 'Added.', 4, 1),
(41, '2016-06-05 06:48:03.135356', '9', '马十', 1, 'Added.', 9, 1),
(42, '2016-06-05 06:49:12.910384', '11', 'tangba', 1, 'Added.', 4, 1),
(43, '2016-06-05 06:49:58.341170', '10', '唐八', 1, 'Added.', 9, 1),
(44, '2016-06-05 07:24:20.310488', '1', '华山论剑之iOS自定义选择菜单弹窗', 1, 'Added.', 11, 1),
(45, '2016-06-05 07:24:26.123581', '1', '华山论剑之iOS自定义选择菜单弹窗', 2, '没有字段被修改。', 11, 1),
(46, '2016-06-05 07:24:49.503947', '2', 'DualPivotQuickSort 双轴快速排序 源码 笔记', 1, 'Added.', 11, 1),
(47, '2016-06-05 07:25:10.518329', '3', 'Facebook F8App-ReactNative项目源码分析4-js篇', 1, 'Added.', 11, 1),
(48, '2016-06-05 07:25:37.770671', '4', 'XML数据解析', 1, 'Added.', 11, 1),
(49, '2016-06-05 07:26:14.323728', '5', '亲自动手写一个Python库（一）', 1, 'Added.', 11, 1),
(50, '2016-06-05 07:26:47.536487', '6', '快速排序（Java 版本 文字描述+排序过程的截图）', 1, 'Added.', 11, 1),
(51, '2016-06-05 07:27:19.891116', '7', 'Lambda表达式语法简介', 1, 'Added.', 11, 1),
(52, '2016-06-05 07:28:11.056436', '8', '自适应导航条js原生版', 1, 'Added.', 11, 1),
(53, '2016-06-05 07:29:44.368933', '9', '人工智能60年，后深度学习时代关键技术进展', 1, 'Added.', 11, 1),
(54, '2016-06-05 07:30:27.182747', '10', '微软向小米出售1500项专利，建立长期合作伙伴关系', 1, 'Added.', 11, 1),
(55, '2016-06-05 07:30:56.604262', '11', '阿里云Apsara Stack：把万台集群技术塞进自有数据中心', 1, 'Added.', 11, 1),
(56, '2016-06-05 07:31:21.556099', '12', 'Apache Spark 2.0前瞻：为机器学习模型注入持久性', 1, 'Added.', 11, 1),
(57, '2016-06-05 07:31:41.064992', '13', '深度学习指南：基于Ubuntu从头开始搭建环境', 1, 'Added.', 11, 1),
(58, '2016-06-05 07:32:27.310729', '14', '数据立法：你的隐私谁来保护？', 1, 'Added.', 11, 1),
(59, '2016-06-05 07:32:52.856186', '15', '业务至上', 1, 'Added.', 11, 1),
(60, '2016-06-05 07:33:24.369249', '16', '金融大数据信用评分模型解析', 1, 'Added.', 11, 1),
(61, '2016-06-06 09:59:49.512096', '18', '啦啦啦啦', 3, '', 12, 1),
(62, '2016-06-06 09:59:49.521191', '17', '没有bug', 3, '', 12, 1);

DROP TABLE IF EXISTS `django_content_type`;
CREATE TABLE IF NOT EXISTS `django_content_type` (
  `id` int(11) NOT NULL,
  `app_label` varchar(100) NOT NULL,
  `model` varchar(100) NOT NULL
) ENGINE=InnoDB AUTO_INCREMENT=15 DEFAULT CHARSET=utf8;

INSERT INTO `django_content_type` (`id`, `app_label`, `model`) VALUES
(1, 'admin', 'logentry'),
(3, 'auth', 'group'),
(2, 'auth', 'permission'),
(4, 'auth', 'user'),
(5, 'contenttypes', 'contenttype'),
(10, 'payroll', 'attendrecord'),
(13, 'payroll', 'comment'),
(8, 'payroll', 'department'),
(9, 'payroll', 'employee'),
(7, 'payroll', 'employeetype'),
(11, 'payroll', 'notice'),
(14, 'payroll', 'payroll'),
(12, 'payroll', 'post'),
(6, 'sessions', 'session');

DROP TABLE IF EXISTS `django_migrations`;
CREATE TABLE IF NOT EXISTS `django_migrations` (
  `id` int(11) NOT NULL,
  `app` varchar(255) NOT NULL,
  `name` varchar(255) NOT NULL,
  `applied` datetime(6) NOT NULL
) ENGINE=InnoDB AUTO_INCREMENT=17 DEFAULT CHARSET=utf8;

INSERT INTO `django_migrations` (`id`, `app`, `name`, `applied`) VALUES
(1, 'contenttypes', '0001_initial', '2016-05-31 07:58:24.589389'),
(2, 'auth', '0001_initial', '2016-05-31 07:58:24.912356'),
(3, 'admin', '0001_initial', '2016-05-31 07:58:24.999054'),
(4, 'admin', '0002_logentry_remove_auto_add', '2016-05-31 07:58:25.033445'),
(5, 'contenttypes', '0002_remove_content_type_name', '2016-05-31 07:58:25.153631'),
(6, 'auth', '0002_alter_permission_name_max_length', '2016-05-31 07:58:25.200082'),
(7, 'auth', '0003_alter_user_email_max_length', '2016-05-31 07:58:25.234977'),
(8, 'auth', '0004_alter_user_username_opts', '2016-05-31 07:58:25.262838'),
(9, 'auth', '0005_alter_user_last_login_null', '2016-05-31 07:58:25.301016'),
(10, 'auth', '0006_require_contenttypes_0002', '2016-05-31 07:58:25.317147'),
(11, 'auth', '0007_alter_validators_add_error_messages', '2016-05-31 07:58:25.334990'),
(12, 'sessions', '0001_initial', '2016-05-31 07:58:25.395684'),
(13, 'payroll', '0001_initial', '2016-05-31 07:58:57.057714'),
(14, 'payroll', '0002_auto_20160531_1604', '2016-05-31 08:04:07.585168'),
(15, 'payroll', '0003_auto_20160531_1613', '2016-05-31 08:13:50.286348'),
(16, 'payroll', '0004_auto_20160531_1616', '2016-05-31 08:16:30.913931');

DROP TABLE IF EXISTS `django_session`;
CREATE TABLE IF NOT EXISTS `django_session` (
  `session_key` varchar(40) NOT NULL,
  `session_data` longtext NOT NULL,
  `expire_date` datetime(6) NOT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

INSERT INTO `django_session` (`session_key`, `session_data`, `expire_date`) VALUES
('hykglsfvemx5xz05er9b8fzffk2uczgo', 'MDFiNmI2MmE2NGI3ZDY4YzRjMDE0NDJkODI2ZGNiY2MwNDM2MjEwMjp7Il9hdXRoX3VzZXJfaGFzaCI6IjE2NTIwNmJkYjcxMzRjYmE2ZjBhNzFlMDBmMmI5YWM2MDA0OTc2OTAiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIyIn0=', '2016-06-20 10:04:02.963882');

DROP TABLE IF EXISTS `payroll_attendrecord`;
CREATE TABLE IF NOT EXISTS `payroll_attendrecord` (
  `id` int(11) NOT NULL,
  `date` date NOT NULL,
  `start` time(6) NOT NULL,
  `end` time(6) NOT NULL,
  `hour` double DEFAULT NULL,
  `employee_id` int(11) NOT NULL
) ENGINE=InnoDB AUTO_INCREMENT=456 DEFAULT CHARSET=utf8;

INSERT INTO `payroll_attendrecord` (`id`, `date`, `start`, `end`, `hour`, `employee_id`) VALUES
(96, '2016-05-01', '08:00:00.000000', '17:00:00.000000', 9, 1),
(97, '2016-05-02', '09:00:00.000000', '17:00:00.000000', 8, 1),
(98, '2016-05-03', '10:00:00.000000', '16:00:00.000000', 6, 1),
(99, '2016-05-04', '08:00:00.000000', '16:00:00.000000', 8, 1),
(100, '2016-05-05', '08:00:00.000000', '18:00:00.000000', 10, 1),
(101, '2016-05-06', '10:00:00.000000', '17:00:00.000000', 7, 1),
(102, '2016-05-07', '10:00:00.000000', '18:00:00.000000', 8, 1),
(103, '2016-05-08', '08:00:00.000000', '17:00:00.000000', 9, 1),
(104, '2016-05-09', '09:00:00.000000', '18:00:00.000000', 9, 1),
(105, '2016-05-10', '08:00:00.000000', '17:00:00.000000', 9, 1),
(106, '2016-05-11', '09:00:00.000000', '16:00:00.000000', 7, 1),
(107, '2016-05-12', '09:00:00.000000', '17:00:00.000000', 8, 1),
(108, '2016-05-13', '09:00:00.000000', '18:00:00.000000', 9, 1),
(109, '2016-05-14', '08:00:00.000000', '16:00:00.000000', 8, 1),
(110, '2016-05-15', '10:00:00.000000', '16:00:00.000000', 6, 1),
(111, '2016-05-16', '08:00:00.000000', '18:00:00.000000', 10, 1),
(112, '2016-05-17', '10:00:00.000000', '18:00:00.000000', 8, 1),
(113, '2016-05-18', '09:00:00.000000', '18:00:00.000000', 9, 1),
(114, '2016-05-19', '08:00:00.000000', '18:00:00.000000', 10, 1),
(115, '2016-05-20', '08:00:00.000000', '18:00:00.000000', 10, 1),
(116, '2016-05-21', '09:00:00.000000', '17:00:00.000000', 8, 1),
(117, '2016-05-22', '08:00:00.000000', '17:00:00.000000', 9, 1),
(118, '2016-05-23', '09:00:00.000000', '16:00:00.000000', 7, 1),
(119, '2016-05-24', '09:00:00.000000', '17:00:00.000000', 8, 1),
(120, '2016-05-25', '08:00:00.000000', '17:00:00.000000', 9, 1),
(121, '2016-05-26', '09:00:00.000000', '18:00:00.000000', 9, 1),
(122, '2016-05-27', '09:00:00.000000', '17:00:00.000000', 8, 1),
(123, '2016-05-28', '09:00:00.000000', '16:00:00.000000', 7, 1),
(124, '2016-05-29', '08:00:00.000000', '16:00:00.000000', 8, 1),
(125, '2016-05-30', '08:00:00.000000', '16:00:00.000000', 8, 1),
(126, '2016-04-01', '10:00:00.000000', '18:00:00.000000', 8, 1),
(127, '2016-04-02', '08:00:00.000000', '17:00:00.000000', 9, 1),
(128, '2016-04-03', '08:00:00.000000', '16:00:00.000000', 8, 1),
(129, '2016-04-04', '09:00:00.000000', '16:00:00.000000', 7, 1),
(130, '2016-04-05', '09:00:00.000000', '18:00:00.000000', 9, 1),
(131, '2016-04-06', '09:00:00.000000', '17:00:00.000000', 8, 1),
(132, '2016-04-07', '09:00:00.000000', '16:00:00.000000', 7, 1),
(133, '2016-04-08', '08:00:00.000000', '18:00:00.000000', 10, 1),
(134, '2016-04-09', '08:00:00.000000', '18:00:00.000000', 10, 1),
(135, '2016-04-10', '09:00:00.000000', '16:00:00.000000', 7, 1),
(136, '2016-04-11', '10:00:00.000000', '16:00:00.000000', 6, 1),
(137, '2016-04-12', '10:00:00.000000', '17:00:00.000000', 7, 1),
(138, '2016-04-13', '08:00:00.000000', '17:00:00.000000', 9, 1),
(139, '2016-04-14', '09:00:00.000000', '17:00:00.000000', 8, 1),
(140, '2016-04-15', '09:00:00.000000', '16:00:00.000000', 7, 1),
(141, '2016-04-16', '08:00:00.000000', '17:00:00.000000', 9, 1),
(142, '2016-04-17', '09:00:00.000000', '17:00:00.000000', 8, 1),
(143, '2016-04-18', '08:00:00.000000', '17:00:00.000000', 9, 1),
(144, '2016-04-19', '08:00:00.000000', '16:00:00.000000', 8, 1),
(145, '2016-04-20', '08:00:00.000000', '17:00:00.000000', 9, 1),
(146, '2016-04-21', '09:00:00.000000', '17:00:00.000000', 8, 1),
(147, '2016-04-22', '09:00:00.000000', '17:00:00.000000', 8, 1),
(148, '2016-04-23', '10:00:00.000000', '17:00:00.000000', 7, 1),
(149, '2016-04-24', '08:00:00.000000', '17:00:00.000000', 9, 1),
(150, '2016-04-25', '09:00:00.000000', '16:00:00.000000', 7, 1),
(151, '2016-04-26', '08:00:00.000000', '18:00:00.000000', 10, 1),
(152, '2016-04-27', '08:00:00.000000', '16:00:00.000000', 8, 1),
(153, '2016-04-28', '08:00:00.000000', '16:00:00.000000', 8, 1),
(154, '2016-04-29', '10:00:00.000000', '18:00:00.000000', 8, 1),
(155, '2016-04-30', '10:00:00.000000', '16:00:00.000000', 6, 1),
(156, '2016-04-01', '10:00:00.000000', '18:00:00.000000', 8, 2),
(157, '2016-04-02', '09:00:00.000000', '16:00:00.000000', 7, 2),
(158, '2016-04-03', '09:00:00.000000', '17:00:00.000000', 8, 2),
(159, '2016-04-04', '08:00:00.000000', '16:00:00.000000', 8, 2),
(160, '2016-04-05', '08:00:00.000000', '18:00:00.000000', 10, 2),
(161, '2016-04-06', '08:00:00.000000', '17:00:00.000000', 9, 2),
(162, '2016-04-07', '09:00:00.000000', '18:00:00.000000', 9, 2),
(163, '2016-04-08', '10:00:00.000000', '18:00:00.000000', 8, 2),
(164, '2016-04-09', '08:00:00.000000', '17:00:00.000000', 9, 2),
(165, '2016-04-10', '08:00:00.000000', '17:00:00.000000', 9, 2),
(166, '2016-04-11', '10:00:00.000000', '18:00:00.000000', 8, 2),
(167, '2016-04-12', '10:00:00.000000', '16:00:00.000000', 6, 2),
(168, '2016-04-13', '10:00:00.000000', '16:00:00.000000', 6, 2),
(169, '2016-04-14', '10:00:00.000000', '17:00:00.000000', 7, 2),
(170, '2016-04-15', '08:00:00.000000', '17:00:00.000000', 9, 2),
(171, '2016-04-16', '08:00:00.000000', '16:00:00.000000', 8, 2),
(172, '2016-04-17', '10:00:00.000000', '16:00:00.000000', 6, 2),
(173, '2016-04-18', '09:00:00.000000', '16:00:00.000000', 7, 2),
(174, '2016-04-19', '10:00:00.000000', '18:00:00.000000', 8, 2),
(175, '2016-04-20', '10:00:00.000000', '17:00:00.000000', 7, 2),
(176, '2016-04-21', '10:00:00.000000', '17:00:00.000000', 7, 2),
(177, '2016-04-22', '08:00:00.000000', '16:00:00.000000', 8, 2),
(178, '2016-04-23', '08:00:00.000000', '18:00:00.000000', 10, 2),
(179, '2016-04-24', '10:00:00.000000', '16:00:00.000000', 6, 2),
(180, '2016-04-25', '09:00:00.000000', '18:00:00.000000', 9, 2),
(181, '2016-04-26', '09:00:00.000000', '18:00:00.000000', 9, 2),
(182, '2016-04-27', '10:00:00.000000', '16:00:00.000000', 6, 2),
(183, '2016-04-28', '09:00:00.000000', '18:00:00.000000', 9, 2),
(184, '2016-04-29', '08:00:00.000000', '18:00:00.000000', 10, 2),
(185, '2016-04-30', '08:00:00.000000', '16:00:00.000000', 8, 2),
(186, '2016-05-01', '10:00:00.000000', '17:00:00.000000', 7, 2),
(187, '2016-05-02', '08:00:00.000000', '17:00:00.000000', 9, 2),
(188, '2016-05-03', '09:00:00.000000', '17:00:00.000000', 8, 2),
(189, '2016-05-04', '08:00:00.000000', '18:00:00.000000', 10, 2),
(190, '2016-05-05', '08:00:00.000000', '16:00:00.000000', 8, 2),
(191, '2016-05-06', '08:00:00.000000', '18:00:00.000000', 10, 2),
(192, '2016-05-07', '08:00:00.000000', '17:00:00.000000', 9, 2),
(193, '2016-05-08', '08:00:00.000000', '17:00:00.000000', 9, 2),
(194, '2016-05-09', '08:00:00.000000', '16:00:00.000000', 8, 2),
(195, '2016-05-10', '08:00:00.000000', '17:00:00.000000', 9, 2),
(196, '2016-05-11', '08:00:00.000000', '18:00:00.000000', 10, 2),
(197, '2016-05-12', '10:00:00.000000', '16:00:00.000000', 6, 2),
(198, '2016-05-13', '08:00:00.000000', '16:00:00.000000', 8, 2),
(199, '2016-05-14', '09:00:00.000000', '16:00:00.000000', 7, 2),
(200, '2016-05-15', '08:00:00.000000', '18:00:00.000000', 10, 2),
(201, '2016-05-16', '10:00:00.000000', '17:00:00.000000', 7, 2),
(202, '2016-05-17', '09:00:00.000000', '18:00:00.000000', 9, 2),
(203, '2016-05-18', '10:00:00.000000', '17:00:00.000000', 7, 2),
(204, '2016-05-19', '10:00:00.000000', '18:00:00.000000', 8, 2),
(205, '2016-05-20', '09:00:00.000000', '16:00:00.000000', 7, 2),
(206, '2016-05-21', '09:00:00.000000', '16:00:00.000000', 7, 2),
(207, '2016-05-22', '10:00:00.000000', '18:00:00.000000', 8, 2),
(208, '2016-05-23', '09:00:00.000000', '17:00:00.000000', 8, 2),
(209, '2016-05-24', '09:00:00.000000', '16:00:00.000000', 7, 2),
(210, '2016-05-25', '10:00:00.000000', '18:00:00.000000', 8, 2),
(211, '2016-05-26', '10:00:00.000000', '17:00:00.000000', 7, 2),
(212, '2016-05-27', '09:00:00.000000', '17:00:00.000000', 8, 2),
(213, '2016-05-28', '10:00:00.000000', '18:00:00.000000', 8, 2),
(214, '2016-05-29', '10:00:00.000000', '17:00:00.000000', 7, 2),
(215, '2016-05-30', '08:00:00.000000', '17:00:00.000000', 9, 2),
(216, '2016-05-01', '08:00:00.000000', '19:00:00.000000', 11, 3),
(217, '2016-05-02', '08:00:00.000000', '18:00:00.000000', 10, 3),
(218, '2016-05-03', '09:00:00.000000', '18:00:00.000000', 9, 3),
(219, '2016-05-04', '10:00:00.000000', '19:00:00.000000', 9, 3),
(220, '2016-05-05', '09:00:00.000000', '18:00:00.000000', 9, 3),
(221, '2016-05-06', '10:00:00.000000', '17:00:00.000000', 7, 3),
(222, '2016-05-07', '09:00:00.000000', '18:00:00.000000', 9, 3),
(223, '2016-05-08', '08:00:00.000000', '19:00:00.000000', 11, 3),
(224, '2016-05-09', '09:00:00.000000', '17:00:00.000000', 8, 3),
(225, '2016-05-10', '10:00:00.000000', '19:00:00.000000', 9, 3),
(226, '2016-05-11', '09:00:00.000000', '17:00:00.000000', 8, 3),
(227, '2016-05-12', '10:00:00.000000', '19:00:00.000000', 9, 3),
(228, '2016-05-13', '10:00:00.000000', '19:00:00.000000', 9, 3),
(229, '2016-05-14', '08:00:00.000000', '19:00:00.000000', 11, 3),
(230, '2016-05-15', '10:00:00.000000', '18:00:00.000000', 8, 3),
(231, '2016-05-16', '10:00:00.000000', '19:00:00.000000', 9, 3),
(232, '2016-05-17', '09:00:00.000000', '19:00:00.000000', 10, 3),
(233, '2016-05-18', '09:00:00.000000', '18:00:00.000000', 9, 3),
(234, '2016-05-19', '09:00:00.000000', '19:00:00.000000', 10, 3),
(235, '2016-05-20', '09:00:00.000000', '19:00:00.000000', 10, 3),
(236, '2016-05-21', '10:00:00.000000', '17:00:00.000000', 7, 3),
(237, '2016-05-22', '10:00:00.000000', '18:00:00.000000', 8, 3),
(238, '2016-05-23', '08:00:00.000000', '18:00:00.000000', 10, 3),
(239, '2016-05-24', '10:00:00.000000', '19:00:00.000000', 9, 3),
(240, '2016-05-25', '09:00:00.000000', '18:00:00.000000', 9, 3),
(241, '2016-05-26', '08:00:00.000000', '18:00:00.000000', 10, 3),
(242, '2016-05-27', '10:00:00.000000', '17:00:00.000000', 7, 3),
(243, '2016-05-28', '08:00:00.000000', '19:00:00.000000', 11, 3),
(244, '2016-05-29', '08:00:00.000000', '18:00:00.000000', 10, 3),
(245, '2016-05-30', '10:00:00.000000', '17:00:00.000000', 7, 3),
(246, '2016-05-01', '08:00:00.000000', '18:00:00.000000', 10, 4),
(247, '2016-05-02', '08:00:00.000000', '19:00:00.000000', 11, 4),
(248, '2016-05-03', '09:00:00.000000', '19:00:00.000000', 10, 4),
(249, '2016-05-04', '08:00:00.000000', '19:00:00.000000', 11, 4),
(250, '2016-05-05', '09:00:00.000000', '17:00:00.000000', 8, 4),
(251, '2016-05-06', '10:00:00.000000', '19:00:00.000000', 9, 4),
(252, '2016-05-07', '10:00:00.000000', '19:00:00.000000', 9, 4),
(253, '2016-05-08', '08:00:00.000000', '17:00:00.000000', 9, 4),
(254, '2016-05-09', '09:00:00.000000', '17:00:00.000000', 8, 4),
(255, '2016-05-10', '08:00:00.000000', '18:00:00.000000', 10, 4),
(256, '2016-05-11', '10:00:00.000000', '19:00:00.000000', 9, 4),
(257, '2016-05-12', '09:00:00.000000', '19:00:00.000000', 10, 4),
(258, '2016-05-13', '10:00:00.000000', '19:00:00.000000', 9, 4),
(259, '2016-05-14', '09:00:00.000000', '19:00:00.000000', 10, 4),
(260, '2016-05-15', '10:00:00.000000', '19:00:00.000000', 9, 4),
(261, '2016-05-16', '08:00:00.000000', '18:00:00.000000', 10, 4),
(262, '2016-05-17', '10:00:00.000000', '19:00:00.000000', 9, 4),
(263, '2016-05-18', '08:00:00.000000', '19:00:00.000000', 11, 4),
(264, '2016-05-19', '08:00:00.000000', '19:00:00.000000', 11, 4),
(265, '2016-05-20', '10:00:00.000000', '17:00:00.000000', 7, 4),
(266, '2016-05-21', '10:00:00.000000', '19:00:00.000000', 9, 4),
(267, '2016-05-22', '08:00:00.000000', '19:00:00.000000', 11, 4),
(268, '2016-05-23', '10:00:00.000000', '19:00:00.000000', 9, 4),
(269, '2016-05-24', '09:00:00.000000', '18:00:00.000000', 9, 4),
(270, '2016-05-25', '08:00:00.000000', '18:00:00.000000', 10, 4),
(271, '2016-05-26', '08:00:00.000000', '17:00:00.000000', 9, 4),
(272, '2016-05-27', '10:00:00.000000', '19:00:00.000000', 9, 4),
(273, '2016-05-28', '09:00:00.000000', '19:00:00.000000', 10, 4),
(274, '2016-05-29', '08:00:00.000000', '18:00:00.000000', 10, 4),
(275, '2016-05-30', '09:00:00.000000', '19:00:00.000000', 10, 4),
(276, '2016-05-01', '08:00:00.000000', '17:00:00.000000', 9, 5),
(277, '2016-05-02', '10:00:00.000000', '18:00:00.000000', 8, 5),
(278, '2016-05-03', '10:00:00.000000', '18:00:00.000000', 8, 5),
(279, '2016-05-04', '10:00:00.000000', '19:00:00.000000', 9, 5),
(280, '2016-05-05', '09:00:00.000000', '17:00:00.000000', 8, 5),
(281, '2016-05-06', '09:00:00.000000', '17:00:00.000000', 8, 5),
(282, '2016-05-07', '09:00:00.000000', '17:00:00.000000', 8, 5),
(283, '2016-05-08', '10:00:00.000000', '18:00:00.000000', 8, 5),
(284, '2016-05-09', '10:00:00.000000', '17:00:00.000000', 7, 5),
(285, '2016-05-10', '08:00:00.000000', '19:00:00.000000', 11, 5),
(286, '2016-05-11', '08:00:00.000000', '17:00:00.000000', 9, 5),
(287, '2016-05-12', '10:00:00.000000', '17:00:00.000000', 7, 5),
(288, '2016-05-13', '09:00:00.000000', '19:00:00.000000', 10, 5),
(289, '2016-05-14', '09:00:00.000000', '17:00:00.000000', 8, 5),
(290, '2016-05-15', '09:00:00.000000', '19:00:00.000000', 10, 5),
(291, '2016-05-16', '08:00:00.000000', '18:00:00.000000', 10, 5),
(292, '2016-05-17', '09:00:00.000000', '18:00:00.000000', 9, 5),
(293, '2016-05-18', '10:00:00.000000', '17:00:00.000000', 7, 5),
(294, '2016-05-19', '10:00:00.000000', '19:00:00.000000', 9, 5),
(295, '2016-05-20', '09:00:00.000000', '19:00:00.000000', 10, 5),
(296, '2016-05-21', '09:00:00.000000', '17:00:00.000000', 8, 5),
(297, '2016-05-22', '10:00:00.000000', '19:00:00.000000', 9, 5),
(298, '2016-05-23', '09:00:00.000000', '19:00:00.000000', 10, 5),
(299, '2016-05-24', '10:00:00.000000', '19:00:00.000000', 9, 5),
(300, '2016-05-25', '08:00:00.000000', '17:00:00.000000', 9, 5),
(301, '2016-05-26', '09:00:00.000000', '19:00:00.000000', 10, 5),
(302, '2016-05-27', '08:00:00.000000', '18:00:00.000000', 10, 5),
(303, '2016-05-28', '10:00:00.000000', '19:00:00.000000', 9, 5),
(304, '2016-05-29', '09:00:00.000000', '17:00:00.000000', 8, 5),
(305, '2016-05-30', '10:00:00.000000', '18:00:00.000000', 8, 5),
(306, '2016-05-01', '08:00:00.000000', '17:00:00.000000', 9, 6),
(307, '2016-05-02', '10:00:00.000000', '17:00:00.000000', 7, 6),
(308, '2016-05-03', '10:00:00.000000', '19:00:00.000000', 9, 6),
(309, '2016-05-04', '08:00:00.000000', '18:00:00.000000', 10, 6),
(310, '2016-05-05', '08:00:00.000000', '19:00:00.000000', 11, 6),
(311, '2016-05-06', '10:00:00.000000', '19:00:00.000000', 9, 6),
(312, '2016-05-07', '08:00:00.000000', '17:00:00.000000', 9, 6),
(313, '2016-05-08', '08:00:00.000000', '19:00:00.000000', 11, 6),
(314, '2016-05-09', '08:00:00.000000', '19:00:00.000000', 11, 6),
(315, '2016-05-10', '09:00:00.000000', '19:00:00.000000', 10, 6),
(316, '2016-05-11', '10:00:00.000000', '19:00:00.000000', 9, 6),
(317, '2016-05-12', '09:00:00.000000', '19:00:00.000000', 10, 6),
(318, '2016-05-13', '09:00:00.000000', '18:00:00.000000', 9, 6),
(319, '2016-05-14', '09:00:00.000000', '18:00:00.000000', 9, 6),
(320, '2016-05-15', '08:00:00.000000', '19:00:00.000000', 11, 6),
(321, '2016-05-16', '08:00:00.000000', '19:00:00.000000', 11, 6),
(322, '2016-05-17', '08:00:00.000000', '19:00:00.000000', 11, 6),
(323, '2016-05-18', '10:00:00.000000', '18:00:00.000000', 8, 6),
(324, '2016-05-19', '09:00:00.000000', '18:00:00.000000', 9, 6),
(325, '2016-05-20', '10:00:00.000000', '17:00:00.000000', 7, 6),
(326, '2016-05-21', '08:00:00.000000', '19:00:00.000000', 11, 6),
(327, '2016-05-22', '09:00:00.000000', '19:00:00.000000', 10, 6),
(328, '2016-05-23', '09:00:00.000000', '17:00:00.000000', 8, 6),
(329, '2016-05-24', '09:00:00.000000', '19:00:00.000000', 10, 6),
(330, '2016-05-25', '10:00:00.000000', '17:00:00.000000', 7, 6),
(331, '2016-05-26', '08:00:00.000000', '18:00:00.000000', 10, 6),
(332, '2016-05-27', '10:00:00.000000', '18:00:00.000000', 8, 6),
(333, '2016-05-28', '10:00:00.000000', '17:00:00.000000', 7, 6),
(334, '2016-05-29', '10:00:00.000000', '17:00:00.000000', 7, 6),
(335, '2016-05-30', '09:00:00.000000', '19:00:00.000000', 10, 6),
(336, '2016-05-01', '10:00:00.000000', '17:00:00.000000', 7, 7),
(337, '2016-05-02', '08:00:00.000000', '17:00:00.000000', 9, 7),
(338, '2016-05-03', '09:00:00.000000', '19:00:00.000000', 10, 7),
(339, '2016-05-04', '08:00:00.000000', '19:00:00.000000', 11, 7),
(340, '2016-05-05', '09:00:00.000000', '19:00:00.000000', 10, 7),
(341, '2016-05-06', '08:00:00.000000', '18:00:00.000000', 10, 7),
(342, '2016-05-07', '08:00:00.000000', '19:00:00.000000', 11, 7),
(343, '2016-05-08', '08:00:00.000000', '18:00:00.000000', 10, 7),
(344, '2016-05-09', '10:00:00.000000', '17:00:00.000000', 7, 7),
(345, '2016-05-10', '10:00:00.000000', '18:00:00.000000', 8, 7),
(346, '2016-05-11', '09:00:00.000000', '18:00:00.000000', 9, 7),
(347, '2016-05-12', '10:00:00.000000', '17:00:00.000000', 7, 7),
(348, '2016-05-13', '09:00:00.000000', '17:00:00.000000', 8, 7),
(349, '2016-05-14', '10:00:00.000000', '19:00:00.000000', 9, 7),
(350, '2016-05-15', '09:00:00.000000', '18:00:00.000000', 9, 7),
(351, '2016-05-16', '09:00:00.000000', '17:00:00.000000', 8, 7),
(352, '2016-05-17', '10:00:00.000000', '19:00:00.000000', 9, 7),
(353, '2016-05-18', '08:00:00.000000', '17:00:00.000000', 9, 7),
(354, '2016-05-19', '08:00:00.000000', '18:00:00.000000', 10, 7),
(355, '2016-05-20', '08:00:00.000000', '18:00:00.000000', 10, 7),
(356, '2016-05-21', '09:00:00.000000', '17:00:00.000000', 8, 7),
(357, '2016-05-22', '09:00:00.000000', '19:00:00.000000', 10, 7),
(358, '2016-05-23', '10:00:00.000000', '18:00:00.000000', 8, 7),
(359, '2016-05-24', '09:00:00.000000', '17:00:00.000000', 8, 7),
(360, '2016-05-25', '08:00:00.000000', '17:00:00.000000', 9, 7),
(361, '2016-05-26', '08:00:00.000000', '18:00:00.000000', 10, 7),
(362, '2016-05-27', '10:00:00.000000', '19:00:00.000000', 9, 7),
(363, '2016-05-28', '09:00:00.000000', '17:00:00.000000', 8, 7),
(364, '2016-05-29', '09:00:00.000000', '19:00:00.000000', 10, 7),
(365, '2016-05-30', '10:00:00.000000', '17:00:00.000000', 7, 7),
(366, '2016-05-01', '08:00:00.000000', '17:00:00.000000', 9, 8),
(367, '2016-05-02', '10:00:00.000000', '19:00:00.000000', 9, 8),
(368, '2016-05-03', '09:00:00.000000', '18:00:00.000000', 9, 8),
(369, '2016-05-04', '08:00:00.000000', '18:00:00.000000', 10, 8),
(370, '2016-05-05', '08:00:00.000000', '17:00:00.000000', 9, 8),
(371, '2016-05-06', '08:00:00.000000', '18:00:00.000000', 10, 8),
(372, '2016-05-07', '08:00:00.000000', '18:00:00.000000', 10, 8),
(373, '2016-05-08', '08:00:00.000000', '19:00:00.000000', 11, 8),
(374, '2016-05-09', '10:00:00.000000', '17:00:00.000000', 7, 8),
(375, '2016-05-10', '10:00:00.000000', '17:00:00.000000', 7, 8),
(376, '2016-05-11', '08:00:00.000000', '17:00:00.000000', 9, 8),
(377, '2016-05-12', '09:00:00.000000', '17:00:00.000000', 8, 8),
(378, '2016-05-13', '08:00:00.000000', '18:00:00.000000', 10, 8),
(379, '2016-05-14', '08:00:00.000000', '19:00:00.000000', 11, 8),
(380, '2016-05-15', '09:00:00.000000', '18:00:00.000000', 9, 8),
(381, '2016-05-16', '09:00:00.000000', '17:00:00.000000', 8, 8),
(382, '2016-05-17', '10:00:00.000000', '18:00:00.000000', 8, 8),
(383, '2016-05-18', '09:00:00.000000', '19:00:00.000000', 10, 8),
(384, '2016-05-19', '08:00:00.000000', '17:00:00.000000', 9, 8),
(385, '2016-05-20', '09:00:00.000000', '18:00:00.000000', 9, 8),
(386, '2016-05-21', '10:00:00.000000', '18:00:00.000000', 8, 8),
(387, '2016-05-22', '08:00:00.000000', '17:00:00.000000', 9, 8),
(388, '2016-05-23', '10:00:00.000000', '17:00:00.000000', 7, 8),
(389, '2016-05-24', '10:00:00.000000', '19:00:00.000000', 9, 8),
(390, '2016-05-25', '08:00:00.000000', '18:00:00.000000', 10, 8),
(391, '2016-05-26', '10:00:00.000000', '18:00:00.000000', 8, 8),
(392, '2016-05-27', '08:00:00.000000', '17:00:00.000000', 9, 8),
(393, '2016-05-28', '09:00:00.000000', '18:00:00.000000', 9, 8),
(394, '2016-05-29', '08:00:00.000000', '19:00:00.000000', 11, 8),
(395, '2016-05-30', '09:00:00.000000', '17:00:00.000000', 8, 8),
(396, '2016-05-01', '09:00:00.000000', '19:00:00.000000', 10, 9),
(397, '2016-05-02', '08:00:00.000000', '19:00:00.000000', 11, 9),
(398, '2016-05-03', '09:00:00.000000', '18:00:00.000000', 9, 9),
(399, '2016-05-04', '10:00:00.000000', '18:00:00.000000', 8, 9),
(400, '2016-05-05', '08:00:00.000000', '18:00:00.000000', 10, 9),
(401, '2016-05-06', '10:00:00.000000', '18:00:00.000000', 8, 9),
(402, '2016-05-07', '08:00:00.000000', '19:00:00.000000', 11, 9),
(403, '2016-05-08', '10:00:00.000000', '18:00:00.000000', 8, 9),
(404, '2016-05-09', '09:00:00.000000', '17:00:00.000000', 8, 9),
(405, '2016-05-10', '10:00:00.000000', '18:00:00.000000', 8, 9),
(406, '2016-05-11', '08:00:00.000000', '19:00:00.000000', 11, 9),
(407, '2016-05-12', '08:00:00.000000', '18:00:00.000000', 10, 9),
(408, '2016-05-13', '09:00:00.000000', '17:00:00.000000', 8, 9),
(409, '2016-05-14', '09:00:00.000000', '19:00:00.000000', 10, 9),
(410, '2016-05-15', '10:00:00.000000', '17:00:00.000000', 7, 9),
(411, '2016-05-16', '10:00:00.000000', '19:00:00.000000', 9, 9),
(412, '2016-05-17', '10:00:00.000000', '19:00:00.000000', 9, 9),
(413, '2016-05-18', '10:00:00.000000', '19:00:00.000000', 9, 9),
(414, '2016-05-19', '08:00:00.000000', '18:00:00.000000', 10, 9),
(415, '2016-05-20', '10:00:00.000000', '19:00:00.000000', 9, 9),
(416, '2016-05-21', '08:00:00.000000', '17:00:00.000000', 9, 9),
(417, '2016-05-22', '09:00:00.000000', '18:00:00.000000', 9, 9),
(418, '2016-05-23', '09:00:00.000000', '18:00:00.000000', 9, 9),
(419, '2016-05-24', '09:00:00.000000', '17:00:00.000000', 8, 9),
(420, '2016-05-25', '09:00:00.000000', '19:00:00.000000', 10, 9),
(421, '2016-05-26', '08:00:00.000000', '18:00:00.000000', 10, 9),
(422, '2016-05-27', '09:00:00.000000', '17:00:00.000000', 8, 9),
(423, '2016-05-28', '10:00:00.000000', '17:00:00.000000', 7, 9),
(424, '2016-05-29', '08:00:00.000000', '18:00:00.000000', 10, 9),
(425, '2016-05-30', '08:00:00.000000', '18:00:00.000000', 10, 9),
(426, '2016-05-01', '08:00:00.000000', '17:00:00.000000', 9, 10),
(427, '2016-05-02', '10:00:00.000000', '18:00:00.000000', 8, 10),
(428, '2016-05-03', '10:00:00.000000', '19:00:00.000000', 9, 10),
(429, '2016-05-04', '09:00:00.000000', '18:00:00.000000', 9, 10),
(430, '2016-05-05', '08:00:00.000000', '19:00:00.000000', 11, 10),
(431, '2016-05-06', '08:00:00.000000', '17:00:00.000000', 9, 10),
(432, '2016-05-07', '08:00:00.000000', '18:00:00.000000', 10, 10),
(433, '2016-05-08', '09:00:00.000000', '19:00:00.000000', 10, 10),
(434, '2016-05-09', '10:00:00.000000', '18:00:00.000000', 8, 10),
(435, '2016-05-10', '08:00:00.000000', '19:00:00.000000', 11, 10),
(436, '2016-05-11', '09:00:00.000000', '18:00:00.000000', 9, 10),
(437, '2016-05-12', '10:00:00.000000', '17:00:00.000000', 7, 10),
(438, '2016-05-13', '10:00:00.000000', '18:00:00.000000', 8, 10),
(439, '2016-05-14', '08:00:00.000000', '18:00:00.000000', 10, 10),
(440, '2016-05-15', '09:00:00.000000', '19:00:00.000000', 10, 10),
(441, '2016-05-16', '10:00:00.000000', '19:00:00.000000', 9, 10),
(442, '2016-05-17', '10:00:00.000000', '18:00:00.000000', 8, 10),
(443, '2016-05-18', '10:00:00.000000', '19:00:00.000000', 9, 10),
(444, '2016-05-19', '08:00:00.000000', '17:00:00.000000', 9, 10),
(445, '2016-05-20', '08:00:00.000000', '18:00:00.000000', 10, 10),
(446, '2016-05-21', '10:00:00.000000', '17:00:00.000000', 7, 10),
(447, '2016-05-22', '09:00:00.000000', '17:00:00.000000', 8, 10),
(448, '2016-05-23', '10:00:00.000000', '19:00:00.000000', 9, 10),
(449, '2016-05-24', '10:00:00.000000', '17:00:00.000000', 7, 10),
(450, '2016-05-25', '10:00:00.000000', '17:00:00.000000', 7, 10),
(451, '2016-05-26', '10:00:00.000000', '17:00:00.000000', 7, 10),
(452, '2016-05-27', '08:00:00.000000', '18:00:00.000000', 10, 10),
(453, '2016-05-28', '08:00:00.000000', '19:00:00.000000', 11, 10),
(454, '2016-05-29', '10:00:00.000000', '18:00:00.000000', 8, 10),
(455, '2016-05-30', '08:00:00.000000', '17:00:00.000000', 9, 10);

DROP TABLE IF EXISTS `payroll_comment`;
CREATE TABLE IF NOT EXISTS `payroll_comment` (
  `id` int(11) NOT NULL,
  `pubdate` date NOT NULL,
  `content` longtext NOT NULL,
  `employee_id` int(11) NOT NULL,
  `post_id` int(11) NOT NULL
) ENGINE=InnoDB AUTO_INCREMENT=41 DEFAULT CHARSET=utf8;

INSERT INTO `payroll_comment` (`id`, `pubdate`, `content`, `employee_id`, `post_id`) VALUES
(1, '2016-06-05', '不要崇拜哥，哥只是贴吧里的一阵风（phantom）', 1, 1),
(2, '2016-06-05', '但是离开江苏之后，开始在外用餐，我日常饮食中的那些负卡路里食物逐渐消失了。然而我喜爱肉类和炸串的习惯却没有变，随着在外用餐时日的增加，接触到越来越多高热量食物，最终也就胖了起来。', 2, 2),
(3, '2016-06-05', 'URL中使用JavaScript\r\n在URL后面跟一个javascript:协议限定符，是另一种嵌入JavaScript代码到客户端的方式。javascript: URL可以用在可以使用常规URL的任意地方：比如<a>标记的href属性，<form>的action属性，甚至window.open()方法的参数。\r\n\r\n文／kissLife（简书作者）\r\n原文链接：http://www.jianshu.com/p/c7319e628916\r\n著作权归作者所有，转载请联系作者获得授权，并标注“简书作者”。\r\n', 2, 4),
(4, '2016-06-05', '就像单击一个URL链接，浏览器会擦除当前文档并显示新文档。上例中将显示当地时间。', 2, 1),
(5, '2016-06-05', '有些时候，你必须把某些文件放到Git工作目录中，但又不能提交它们，比如保存了数据库密码的配置文件啦。\r\n在Git工作区的根目录下创建一个特殊的.gitignore文件，然后把要忽略的文件名填进去，Git就会自动忽略这些文件。', 3, 5),
(6, '2016-06-05', '忽略文件的原则是：\r\n\r\n忽略操作系统自动生成的文件，比如缩略图等；\r\n忽略编译生成的中间文件、可执行文件等，也就是如果一个文件是通过另一个文件自动生成的，那自动生成的文件就没必要放进版本库，比如Java编译产生的.class文件；\r\n忽略你自己的带有敏感信息的配置文件，比如存放口令的配置文件。\r\n\r\n文／洛朗不展傅立叶（简书作者）\r\n原文链接：http://www.jianshu.com/p/448b46a49ec6\r\n著作权归作者所有，转载请联系作者获得授权，并标注“简书作者”。', 3, 1),
(7, '2016-06-05', '假设你在Windows下进行Python开发，Windows会自动在有图片的目录下生成隐藏的缩略图文件，如果有自定义目录，目录下就会有Desktop.ini文件，因此你需要忽略Windows自动生成的垃圾文件：', 3, 3),
(8, '2016-06-05', '最后一步就是把.gitignore也提交到Git，就完成了！当然检验.gitignore的标准是git status命令是不是说working directory clean。', 3, 2),
(9, '2016-06-05', '使用Windows的童鞋注意了，如果你在资源管理器里新建一个.gitignore文件，它会非常弱智地提示你必须输入文件名，但是在文本编辑器里“保存”或者“另存为”就可以把文件保存为.gitignore了。', 3, 5),
(10, '2016-06-05', '如果觉得我的文章对您有用，请随意打赏。您的支持将鼓励我继续创作！', 4, 6),
(11, '2016-06-05', '如果觉得我的文章对您有用，请随意打赏。您的支持将鼓励我继续创作！', 4, 6),
(12, '2016-06-05', '如果觉得我的文章对您有用，请随意打赏。您的支持将鼓励我继续创作！', 4, 2),
(13, '2016-06-05', '如果觉得我的文章对您有用，请随意打赏。您的支持将鼓励我继续创作！', 4, 3),
(14, '2016-06-05', '如果觉得我的文章对您有用，请随意打赏。您的支持将鼓励我继续创作！', 4, 4),
(15, '2016-06-05', '河上有一艘船，叫做知识。\r\n\r\n上这艘船很多办法，其中一项就是读书。很多人以为，买了书，读完了，就上了船。\r\n\r\n上了船，自然这艘船就会达到河对面，到达真理之岸。', 5, 1),
(16, '2016-06-05', '河上有一艘船，叫做知识。\r\n\r\n上这艘船很多办法，其中一项就是读书。很多人以为，买了书，读完了，就上了船。\r\n\r\n上了船，自然这艘船就会达到河对面，到达真理之岸。', 5, 5),
(17, '2016-06-05', '河上有一艘船，叫做知识。\r\n\r\n上这艘船很多办法，其中一项就是读书。很多人以为，买了书，读完了，就上了船。\r\n\r\n上了船，自然这艘船就会达到河对面，到达真理之岸。', 5, 6),
(18, '2016-06-05', '1、基本选择器\r\n$("#id")：id选择器，返回单个元素\r\n$(".class")：class选择器，返回集合元素\r\n$("element")：选定指定的元素名匹配的元素，返回集合元素\r\n$("*")：通配符选择器，选择所有元素，返回集合元素\r\n$("selector1,selector2")：选择所有选择器匹配的元素，返回集合元素\r\n\r\n', 6, 2),
(19, '2016-06-05', '1、基本选择器\r\n$("#id")：id选择器，返回单个元素\r\n$(".class")：class选择器，返回集合元素\r\n$("element")：选定指定的元素名匹配的元素，返回集合元素\r\n$("*")：通配符选择器，选择所有元素，返回集合元素\r\n$("selector1,selector2")：选择所有选择器匹配的元素，返回集合元素\r\n', 6, 3),
(20, '2016-06-05', '1、基本选择器\r\n$("#id")：id选择器，返回单个元素\r\n$(".class")：class选择器，返回集合元素\r\n$("element")：选定指定的元素名匹配的元素，返回集合元素\r\n$("*")：通配符选择器，选择所有元素，返回集合元素\r\n$("selector1,selector2")：选择所有选择器匹配的元素，返回集合元素\r\n', 6, 4),
(21, '2016-06-05', '1、基本选择器\r\n$("#id")：id选择器，返回单个元素\r\n$(".class")：class选择器，返回集合元素\r\n$("element")：选定指定的元素名匹配的元素，返回集合元素\r\n$("*")：通配符选择器，选择所有元素，返回集合元素\r\n$("selector1,selector2")：选择所有选择器匹配的元素，返回集合元素\r\n', 6, 7),
(22, '2016-06-05', '1、基本选择器\r\n$("#id")：id选择器，返回单个元素\r\n$(".class")：class选择器，返回集合元素\r\n$("element")：选定指定的元素名匹配的元素，返回集合元素\r\n$("*")：通配符选择器，选择所有元素，返回集合元素\r\n$("selector1,selector2")：选择所有选择器匹配的元素，返回集合元素\r\n', 6, 8),
(23, '2016-06-05', '背景：随着公司相关APP项目的开展，公用框架的创建与维护越发显得迫切起来。因为工作中经常接触使用cocoapods,也知道她其实可以搞定这件事，所以就首当其冲的选择了基于cocoapods的封装方案。', 7, 1),
(24, '2016-06-05', '背景：随着公司相关APP项目的开展，公用框架的创建与维护越发显得迫切起来。因为工作中经常接触使用cocoapods,也知道她其实可以搞定这件事，所以就首当其冲的选择了基于cocoapods的封装方案。', 7, 5),
(25, '2016-06-05', '背景：随着公司相关APP项目的开展，公用框架的创建与维护越发显得迫切起来。因为工作中经常接触使用cocoapods,也知道她其实可以搞定这件事，所以就首当其冲的选择了基于cocoapods的封装方案。', 7, 5),
(26, '2016-06-05', '函数的第一规则是短小', 3, 6),
(27, '2016-06-05', '函数的第一规则是短小', 3, 9),
(28, '2016-06-05', '函数的第一规则是短小', 3, 10),
(29, '2016-06-05', '函数的第一规则是短小', 3, 11),
(30, '2016-06-05', '函数的第一规则是短小', 3, 2),
(31, '2016-06-05', '函数的第一规则是短小', 3, 3),
(32, '2016-06-05', '函数的第一规则是短小', 3, 4),
(33, '2016-06-05', '在作图的时候，不知道大家是否常用快捷键呢？下面是整理的是一些比较常用的3D max快捷键，希望大家都能派上用场。', 3, 16),
(34, '2016-06-05', '在作图的时候，不知道大家是否常用快捷键呢？下面是整理的是一些比较常用的3D max快捷键，希望大家都能派上用场。', 3, 3),
(35, '2016-06-05', '在作图的时候，不知道大家是否常用快捷键呢？下面是整理的是一些比较常用的3D max快捷键，希望大家都能派上用场。', 3, 2),
(36, '2016-06-05', '在作图的时候，不知道大家是否常用快捷键呢？下面是整理的是一些比较常用的3D max快捷键，希望大家都能派上用场。', 3, 5),
(37, '2016-06-05', 'no bug', 3, 5),
(38, '2016-06-05', 'no bug', 1, 5),
(39, '2016-06-06', 'no bug', 1, 5),
(40, '2016-06-06', '没有bug', 1, 5);

DROP TABLE IF EXISTS `payroll_department`;
CREATE TABLE IF NOT EXISTS `payroll_department` (
  `id` int(11) NOT NULL,
  `name` varchar(20) NOT NULL
) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8;

INSERT INTO `payroll_department` (`id`, `name`) VALUES
(2, '人事部'),
(1, '研发部'),
(3, '财务部');

DROP TABLE IF EXISTS `payroll_employee`;
CREATE TABLE IF NOT EXISTS `payroll_employee` (
  `id` int(11) NOT NULL,
  `name` varchar(20) NOT NULL,
  `sex` varchar(1) NOT NULL,
  `phoneNumber` varchar(20) DEFAULT NULL,
  `employeeId` varchar(20) DEFAULT NULL,
  `email` varchar(254) DEFAULT NULL,
  `bankCount` varchar(20) NOT NULL,
  `department_id` int(11) NOT NULL,
  `type_id` int(11) NOT NULL,
  `user_id` int(11) NOT NULL
) ENGINE=InnoDB AUTO_INCREMENT=11 DEFAULT CHARSET=utf8;

INSERT INTO `payroll_employee` (`id`, `name`, `sex`, `phoneNumber`, `employeeId`, `email`, `bankCount`, `department_id`, `type_id`, `user_id`) VALUES
(1, '方品', 'm', '13122222222', '01010001', 'fangpin1993@hotmail.com', '6222021001176635', 1, 1, 2),
(2, '张三', 'm', '13325401789', '02020002', 'zhangsan@hotmail.com', '6222021001117282294', 2, 2, 3),
(3, '李四', 'm', '13332456789', '03030003', 'lisi@hotmail.com', '7228920267628', 3, 3, 4),
(4, '王二', 'm', '13162573329', '01040004', 'anger@hotmail.com', '989278979873897', 1, 4, 5),
(5, '钱五', 'm', '13356789021', '02050005', 'qianwu@hotmail.com', '798890890809089', 2, 5, 6),
(6, '郑六', 'm', '13278882291', '01040006', 'zhengliu@gmail.com', '13987987979789789', 1, 4, 7),
(7, '孙七', 'm', '13233456788', '02020007', 'sunni@gmail.com', '62227788019890890', 2, 2, 8),
(8, '王九', 'm', '13223456623', '03030008', 'wangjiu@hotmail.com', '62220219992242', 3, 3, 9),
(9, '马十', 'm', '13223329733', '01040009', 'mashi@hotmail.com', '72228199202181090', 1, 4, 10),
(10, '唐八', 'm', '13383839921', '03030010', 'tangba@gmail.com', '6622102225436', 3, 3, 11);

DROP TABLE IF EXISTS `payroll_employeetype`;
CREATE TABLE IF NOT EXISTS `payroll_employeetype` (
  `id` int(11) NOT NULL,
  `name` varchar(20) NOT NULL,
  `salaryPerHour` double NOT NULL
) ENGINE=InnoDB AUTO_INCREMENT=6 DEFAULT CHARSET=utf8;

INSERT INTO `payroll_employeetype` (`id`, `name`, `salaryPerHour`) VALUES
(1, '实习', 20),
(2, '高级hr', 100),
(3, '高级会计', 30),
(4, '高级软件工程师', 70),
(5, '部门经理', 140);

DROP TABLE IF EXISTS `payroll_notice`;
CREATE TABLE IF NOT EXISTS `payroll_notice` (
  `id` int(11) NOT NULL,
  `title` varchar(255) NOT NULL,
  `content` longtext NOT NULL,
  `pubdate` date NOT NULL
) ENGINE=InnoDB AUTO_INCREMENT=17 DEFAULT CHARSET=utf8;

INSERT INTO `payroll_notice` (`id`, `title`, `content`, `pubdate`) VALUES
(1, '华山论剑之iOS自定义选择菜单弹窗', '前两天在工作中遇到一个对过的选项进行选择,一开始的时候,我是一个选项一个选项的添加,发现代码的重复率实在是太高了,所以,我就封装了一个方法,这里我对这个方法的参数进行一下讲解\r\ntitleString:本参数是弹窗的标题\r\narray:本参数是一个数组对象,里面存储的是我们所需要的所有的选项,\r\nlabel:这个label对象是我们需要显示我们选择的是那个一个选项,这里我们可以对label进行手势的添加,来实现的这个事件的点击\r\n-(void)addAlertViewControllerWithTitle:(NSString *)titleString menssage:(NSArray *)array showlable:(UILabel *)label{\r\n\r\n    UIAlertController *  alertCtr2 = [UIAlertController alertControllerWithTitle:titleString message:nil preferredStyle:UIAlertControllerStyleAlert];\r\n\r\n    for (int i = 0; i< array.count; i++) {\r\n\r\n        NSString *string = array[i];\r\n        // Create the actions.\r\n        UIAlertAction *ButtonAction = [UIAlertAction actionWithTitle:string style:UIAlertActionStyleDefault handler:^(UIAlertAction *action) {\r\n\r\n            label.text = string;\r\n\r\n        }];\r\n\r\n        [alertCtr2 addAction:ButtonAction];\r\n\r\n    }\r\n\r\n    [self presentViewController:alertCtr2 animated:YES completion:nil];\r\n\r\n}\r\n\r\n文／神经骚栋（简书作者）\r\n原文链接：http://www.jianshu.com/p/9f1649a425f2\r\n著作权归作者所有，转载请联系作者获得授权，并标注“简书作者”。', '2016-06-05'),
(2, 'DualPivotQuickSort 双轴快速排序 源码 笔记', 'DualPivotQuicksort source code\r\n这个算法是Arrays.java中给基本类型的数据排序使用的具体实现。它针对每种基本类型都做了实现，实现的方式有稍微的差异，但是思路都是相同的，所以这里只挑了int类型的排序来看。\r\n\r\n整个实现中的思路是 首先检查数组的长度，比一个阈值小的时候直接使用双轴快排。其它情况下，先检查数组中数据的顺序连续性。把数组中连续升序或者连续降序的信息记录下来，顺便把连续降序的部分倒置。这样数据就被切割成一段段连续升序的数列。\r\n\r\n如果顺序连续性好，直接使用TimSort算法。这个我们之前介绍过，TimSort算法的核心在于利用数列中的原始顺序，所以可以提高很多效率。这里的TimSort算法是之前介绍的TimSort算法的精简版，剪掉了动态阈值的那一部分。\r\n\r\n顺序连续性不好的数组直接使用了 双轴快排 + 成对插入排序。成对插入排序是插入排序的改进版，它采用了同时插入两个元素的方式调高效率。双轴快排是从传统的单轴快排到3-way快排演化过来的，网上之前已经有很多博客介绍这种算法。这里推荐 国外一篇文章，它的3张图和下面的代码帮助我理解了快排，3-way和双轴快排之间的关系。\r\n\r\n代码风格来看感觉不如之前TimSort的代码风格好。代码中的变量命名大部分都是a, b, i, k, j, t这种，让人不好理解。所以建议大家日常写代码也不要使用这种不明含义的命名。最好能做到让其它人一看就懂，比如说用index代替i, 用 temp代替t等等。好在它的核心代码部分注释很全，看起来到不麻烦。\r\n\r\n我把代码的注释贴在下面，有需要的同学自行copy。欢迎大家在评论中一起交流。\r\n\r\n文／于晓飞93（简书作者）\r\n原文链接：http://www.jianshu.com/p/6d26d525bb96\r\n著作权归作者所有，转载请联系作者获得授权，并标注“简书作者”。', '2016-06-05'),
(3, 'Facebook F8App-ReactNative项目源码分析4-js篇', '本文开始分析f8app核心js部分的源码，这篇文章将非常难理解，原因了Redux框架引入了很多新概念，使用了大量函数式编程思想，建议先把后面的参考文章仔细过一遍，确保理解后再看本文。React Native的理念是Learn once,write anywhere, Android和iOS App端的js代码是放在一起的，以便最大限度的复用业务逻辑，UI部分的可以根据平台特性各自实现，React native分别渲染成安卓和iOS的原生UI界面，对于两个平台UI组件的细微差异和完全不同的UI组件2种情况，react native提供了不同的处理方式。\r\n<!-- more -->\r\n\r\njs入口分析\r\nReact Native Android App和iOS App的入口jsbundle对应的默认js源文件分别是index.android.js和index.ios.js，在f8app中这2个文件内容一致。代码如下：\r\n\r\n''use strict'';\r\n\r\nconst {AppRegistry} = require(''react-native'');\r\nconst setup = require(''./js/setup'');\r\n\r\nAppRegistry.registerComponent(''F8v2'', setup);\r\n\r\n文／offbye西涛（简书作者）\r\n原文链接：http://www.jianshu.com/p/28e9c7957d0c\r\n著作权归作者所有，转载请联系作者获得授权，并标注“简书作者”。', '2016-06-05'),
(4, 'XML数据解析', '解析来自服务器返回的XML数据,介绍两种方式:\r\n1,NSXMLParser解析器 : SAX方式解析(从根元素开始,按顺序一个一个的解析,适合解析大文件);\r\n2,GDataXML : DOM方式解析(一次性将整个XML文档加载进内存,适合解析小的文件);\r\n\r\n1 NSXMLParser\r\n1, 确认请求路径\r\n\r\nNSURL *url = [NSURL URLWithString:@"xxx"];\r\n2, 创建会话对象\r\n\r\nNSURLSession *session = [NSURLSession sharedSession];\r\n3, 创建请求Task\r\n\r\nNSURLSessionDataTask *dataTask = [session dataTaskWithURL:url completionHandler:\r\n^(NSData * _Nullable data, NSURLResponse * _Nullable response, NSError * _Nullable error) {\r\n        //创建XML解析器 NSXMLParser(SAX---从根元素一个元素一个元素的解析)\r\n        NSXMLParser *parser = [[NSXMLParser alloc]initWithData:data];\r\n        //设置代理\r\n        parser.delegate = self;\r\n        //开始解析(同步是会阻塞当前执行路径)\r\n        [parser parse];\r\n}];\r\n4,执行Task\r\n\r\n[dataTask resume];\r\n5,代理方法\r\n\r\n//1.当开始解析整个XML文档的时候回调用\r\n- (void)parserDidStartDocument:(NSXMLParser *)parser\r\n{\r\n    NSLog(@"parserDidStartDocument");\r\n}\r\n\r\n//2.开始解析某个元素的时候调用(调用多次)\r\n-(void)parser:(NSXMLParser *)parser didStartElement:(NSString *)elementName \r\nnamespaceURI:(NSString *)namespaceURI qualifiedName:(NSString *)qName attributes:\r\n(NSDictionary<NSString *,NSString *> *)attributeDict\r\n{\r\n    NSLog(@"didStartElement %@元素开始解析--%@",elementName,attributeDict);\r\n}\r\n\r\n//3.某个元素解析完毕的时候调用\r\n- (void)parser:(NSXMLParser *)parser didEndElement:(NSString *)elementName \r\nnamespaceURI:(NSString *)namespaceURI qualifiedName:(NSString *)qName\r\n{\r\n    NSLog(@"didEndElement--%@元素解析完毕",elementName);\r\n}\r\n\r\n//4.整个XML文档解析完毕的时候调用\r\n- (void)parserDidEndDocument:(NSXMLParser *)parser\r\n{\r\n    NSLog(@"parserDidEndDocument");\r\n}\r\n\r\n文／Mr_iOS（简书作者）\r\n原文链接：http://www.jianshu.com/p/008d257bad85\r\n著作权归作者所有，转载请联系作者获得授权，并标注“简书作者”。', '2016-06-05'),
(5, '亲自动手写一个Python库（一）', '引子\r\n学习编程以来，接触过Basic,C/C++,Swift,JavaScript和Python五种语言，其中最爱的当属Python，简洁的语法和丰富的库让我一直沉迷于此，尽管最近实习工作中用的是C++。\r\n\r\n人生苦短，快用Python\r\n最近一年，我将大把的时间投入到学习编程中，收获了满满的成就感，希望未来工作之后，还能保持对编程的爱。学习Python以后，经常感慨开源的伟大，我也一直希望自己能成为这光荣世界的一员，所以趁着工作中需要，利用业余时间开发一个Python库解决CAA开发中遇到的问题。\r\n\r\n需求\r\n从去年十二月份至今，断断续续地接触了DS CAA开发工作，很多人可能并不了解DS CAA。无论是手机家电之类的消费电子产品还是飞机轮船等大型制造产品，在制造之前都需要采用CAD软件进行设计。从上世纪60年代CAD软件开始商用至今，历经技术的不断变革和各家公司的兼并联合，已经形成达索、西门子、PTC“三国争霸”的局面。我实习公司主推的是达索的CAD软件，即CATIA。在CAD领域，我们将利用厂商提供的API在已经成型的CAD软件上增加一些程序（功能）称之为“二次开发”，上面所述的DS CAA就是对CATIA的二次开发。\r\n\r\n\r\nDS CAA采用的开发语言是C++，由于是完全的基于SDK和API的开发，往往开发工作要受限于达索的接口和框架。开发中大量的精力不得不花费在阅读达索API接口文档，查找API，查找模块，查找框架之中，做过类似工作的人一定能理解这其中的个把心酸。在慢慢的工作中，逐渐有了想法，将散落于文档中的API接口用数据库管理，根据自己所写的程序自动添加头文件，模块和框架。当然采用最爱的Python写一个库来解决这一问题，也许并不是最好的解决办法，但我愿意就好，也希望能帮助的苦逼的CAA开发者。\r\n\r\n环境搭建\r\n废话已经说了太多，我们下面将利用PyCharm结合VirtualEnv搭建项目环境。\r\n\r\n安装好Pycharm后，点击configure->Preferecnces进入设置页面，找到Project Interpreter设置选项，该选项显示了当前可用的Python解释器，点击右上角设置图标，然后选择Create VirtualEnv来创建一个Python虚拟环境，虚拟环境的好处是你可以针对自己的虚拟Python环境为所欲为，而不用担心搞坏系统的Python环境，笔者曾经将Mac上的Python环境搞崩溃后，只得默默重装系统。此外，Pycharm还可以方便的帮你管理虚拟环境的Python包，点击左下角➕即可添加库到当前环境中。\r\n\r\n\r\n\r\n\r\n当然，此虚拟环境同样可以通过命令行激活，如我的虚拟环境安装位置为：~/Developer/Python3Env，里面VirtualEnv会帮我们搭建好一整套Python所需的包和执行命令。\r\n\r\n// 激活虚拟环境\r\n$ source ~/Developer/Python3Env/bin/activate\r\n// 退出虚拟环境\r\n$ deactivate\r\n\r\n结下来就用Pycharm创建一个项目，只需注意选择一个虚拟环境即可，我给自己的项目起了个还不错的名“CAAFinder”，项目已经放在GitHub上面，如果有CAA开发者看到，欢迎和我一起完善项目，也欢迎你可以测试它。即使你不是CAA开发者，也希望在Github上面点个赞。\r\n\r\n\r\n\r\n文／Gutierrez（简书作者）\r\n原文链接：http://www.jianshu.com/p/696f1d558ce9\r\n著作权归作者所有，转载请联系作者获得授权，并标注“简书作者”。', '2016-06-05'),
(6, '快速排序（Java 版本 文字描述+排序过程的截图）', '1：概念\r\n     快速排序也叫分治法，它和冒泡排序选择排序不同,每排一次并不是找出最大或者是最小的的数字而是选择一个数字把该数字的左边都比它小（大）并且右边都比它大（小）。\r\n\r\n2：排序过程\r\n如：12 342 1 56 8 789 3 5 999 33 88经过一次排序后12的左边都比12大，12的右边都比12小。如下图所示\r\n\r\n\r\n12的左边都比12大，12的右边都比12小\r\n\r\n     这样无序的数字被分成了3块。12的左边和右边都还可以继续分块。接下来把88当作参考数把 88 342 33 56 999 789 划分成88的左边都比88大，88的右边都比88小.\r\n\r\n\r\n88的左边都比88大，88的右边都比88小\r\n     直到左右都不能划分的时候说明排序结束。附整体流程图\r\n\r\n\r\n整体流程图\r\n3：代码实现要点及程序运行过程截图\r\n     1：一般会有两个参数一个表示数组的开始位置（startIndex），一个表示数组结束位置（endIndex）。\r\n     2：循环结束判断依据为startIndex>=endIndex。\r\n     3：使用一个标志记录排序后该数的位置，使用递归的方式调用排序方法实现快速排序。示例代码为：divideArr(toSort,startIndex,flag-1);divideArr(toSort,flag+1,endIndex);\r\n\r\n程序运行过程截图：\r\n\r\n\r\n程序运行过程截图\r\n4：代码示例\r\npublic static void main(String[] args) {\r\n        /**\r\n         *快速排序(从小到大排序)： 待排序\r\n         */\r\n        int[] toSort={12,342,1,56,8,789,3,5,999,33,88};\r\n\r\n        System.out.print("未排序前结果：");\r\n        printTosort(toSort);\r\n\r\n\r\n        divideArr(toSort,0,toSort.length-1);\r\n\r\n    }\r\n\r\n    /**\r\n     * \r\n     * @param toSort  :待排序的数组\r\n     * @param startIndex ：数组起始位置\r\n     * @param endIndex：数组结束位置\r\n     */\r\n    private static void divideArr(int[] toSort,int startIndex,int endIndex){\r\n\r\n        if(startIndex<endIndex){\r\n            int l=startIndex;\r\n            int r=endIndex;\r\n            int flag=l;//flag:标志位。通过排序后标志位的左边都比它小右边都比它大。\r\n            System.out.println();\r\n            System.out.println("                标志位的值为：【"+toSort[flag]+"】");\r\n            System.out.println();\r\n            int tmp=0;//用来交换时临时存放数的变量。\r\n            while (l<r){\r\n                //从右边找比它小的数\r\n                while(flag<r&&toSort[flag]>toSort[r]){\r\n                    r--;\r\n                }\r\n                if(r>flag){\r\n                    //表示右边存在比标志位还小的数。\r\n                    //1：右边的数和标志位互换。\r\n                    //2：标志位重新标志位右边的下标\r\n                    tmp=toSort[r];\r\n                    toSort[r]=toSort[flag];\r\n                    toSort[flag]=tmp;\r\n                    flag=r;\r\n                }\r\n\r\n                //从左开始边找比它大的数\r\n                while(flag>l&&toSort[flag]<toSort[l]){\r\n                    l++;\r\n                }\r\n\r\n                if(flag>l){\r\n                    //说明左边出现比标志位还大的数。\r\n                    //1：左边的数和标志位互换。\r\n                    //2：标志位重新标志位左边的下标\r\n                    tmp=toSort[l];\r\n                    toSort[l]=toSort[flag];\r\n                    toSort[flag]=tmp;\r\n                    flag=l;\r\n                }\r\n            }\r\n            System.out.print("排序后结果：");\r\n            printTosort(toSort);\r\n\r\n            divideArr(toSort,startIndex,flag-1);\r\n            divideArr(toSort,flag+1,endIndex);\r\n        }\r\n\r\n    }\r\n\r\n\r\n    private static void printTosort(int[] toSort) {\r\n        for(int i=0;i<toSort.length;i++){\r\n            System.out.print(toSort[i]+"   ");\r\n        }\r\n        System.out.println();\r\n    }\r\n\r\n文／平行线（简书作者）\r\n原文链接：http://www.jianshu.com/p/c2c9289db894\r\n著作权归作者所有，转载请联系作者获得授权，并标注“简书作者”。', '2016-06-05'),
(7, 'Lambda表达式语法简介', '初识Lambda已经大致了解Lambda的使用。\r\n其实可以把Java Lambda表达式简单地看做是一个匿名方法，所以他的语法构成就很像一个方法的定义。\r\n\r\npublic class Calculator {\r\n\r\n    interface IntegerMath {\r\n        int operation(int a, int b);   \r\n    }\r\n\r\n    public int operateBinary(int a, int b, IntegerMath op) {\r\n        return op.operation(a, b);\r\n    }\r\n\r\n    public static void main(String... args) {\r\n        Calculator myApp = new Calculator();\r\n        IntegerMath addition = (a, b) -> a + b;\r\n        IntegerMath subtraction = (a, b) -> a - b;\r\n        System.out.println("40 + 2 = " + myApp.operateBinary(40, 2, addition));\r\n        System.out.println("20 - 10 = " + myApp.operateBinary(20, 10, subtraction));    \r\n    }\r\n}\r\naddition和subtraction就可以看做是IntegerMath省略了operation名称的方法。\r\n\r\n表达式组成\r\n括号以及括号里用逗号分隔的参数列表\r\n\r\n仅有一个参数的可以省略括号。\r\n->符号\r\n\r\n花括号以及花括号里的语句\r\n\r\n仅有一条语句时可以省略花括号，并且这条语句的值将作为return返回值。\r\n更严谨的语法规范可以参考Java8语言规范：Lambda表达式。\r\n作用域\r\nLambda的作用域与类和其它匿名类基本一致，也可以进行变量捕捉，但是Lambda不存在类和匿名类的Shadowing问题。\r\n\r\nimport java.util.function.Consumer;\r\n\r\npublic class LambdaScopeTest {\r\n\r\n    public int x = 0;\r\n\r\n    class FirstLevel {\r\n\r\n        public int x = 1;\r\n\r\n        void methodInFirstLevel(int x) {\r\n\r\n            Consumer<Integer> myConsumer = (y) -> {\r\n                System.out.println("x = " + x);\r\n                System.out.println("y = " + y);\r\n                System.out.println("this.x = " + this.x);\r\n                System.out.println("LambdaScopeTest.this.x = " +\r\n                    LambdaScopeTest.this.x);\r\n            };\r\n\r\n            myConsumer.accept(9);\r\n        }\r\n    }\r\n\r\n    public static void main(String... args) {\r\n        LambdaScopeTest st = new LambdaScopeTest();\r\n        LambdaScopeTest.FirstLevel fl = st.new FirstLevel();\r\n        fl.methodInFirstLevel(23);\r\n    }\r\n}\r\n执行结果：\r\n\r\nx = 23\r\ny = 9\r\nthis.x = 1\r\nLambdaScopeTest.this.x = 0\r\nLambda可以访问LambdaScopeTest的成员变量、FirstLevel的成员变量、methodInFirstLevel的参数变量。如果使用Consumer的匿名类，那么内部类FirstLevel成员变量无法直接访问；如果匿名类方法的参数与外部的局部作用域某个变量同名，那么局部作用域的同名变量同样无法访问。\r\n\r\nConsumer<Integer> a = new Consumer<Integer>() {\r\n    @Override\r\n    public void accept(Integer x) {\r\n        System.out.println("x = " + x);\r\n        // System.out.println("this.x = " + this.x); 编译错误，无法访问\r\n        System.out.println("LambdaScopeTest.this.x = " + LambdaScopeTest.this.x);\r\n    }\r\n};\r\n在局部作用域中，Lambda表达式的参数列表是不能与局部作用域中的变量同名，因为Lambda没有像匿名类那样产生新的类作用域。\r\n\r\n在myConsumer这个表达式中定义参数名称为x\r\n\r\nvoid methodInFirstLevel(int x) {\r\n\r\n    Consumer<Integer> myConsumer = (x) -> {\r\n        System.out.println("x = " + x);\r\n        System.out.println("this.x = " + this.x);\r\n        System.out.println("LambdaScopeTest.this.x = " + LambdaScopeTest.this.x);\r\n    };\r\n\r\n    myConsumer.accept(9);\r\n}\r\n由于参数列表中变量名称与局部作用域变量同名，就产生一个编译错误：\r\n\r\nvariable x is already defined in method methodInFirstLevel(int)\r\n如何确定表达式类型\r\n可以把表达式赋值给指定类型的变量\r\n\r\n文／巡山的喽罗（简书作者）\r\n原文链接：http://www.jianshu.com/p/e7db8fddb8b4\r\n著作权归作者所有，转载请联系作者获得授权，并标注“简书作者”。', '2016-06-05'),
(8, '自适应导航条js原生版', '自适应导航条js原生版\r\n看到bootstrap等第三方库中的导航条非常简单好用，没想到自己用原生试着实现用了这么多行代码\r\n\r\n效果如下：\r\n\r\n\r\n实现思路：\r\nhtml+css：\r\n最外层nav宽度100%。高给固定值\r\n中间内容部分全部用一个div包裹，宽度百分比，margin: 0 auto; 实现居中\r\n左侧的logo图片外包div，给的固定宽高\r\n中间的navbar包裹ul列表用的百分比宽度，使其宽度能够在屏幕缩放的时候随之变化\r\n右侧按钮部分隐藏\r\n媒体查询屏幕宽度<768px时，将navbar定位至nav下方，高度给0，并将按钮显示\r\njs：\r\n用布尔值作为开关控制点击状态\r\n先设置bol = true，让判断走if语句\r\n点击一次后，定时器执行navbar和nav的高度增大至预设值，关闭定时器\r\n定时器后将bol改为false，这样下次点击就会执行else中语句\r\nelse中定时器控制nav及navbar高度减小至预设值，关闭定时器\r\n定时器后将bol再改为true，这样下次点击就会执行if中语句\r\n遇到的坑：\r\n遇到的比较棘手的问题就是开关状态、点击事件、定时器到底该如何嵌套，不过最后还是把思路理清楚了\r\n\r\n先用布尔值判定是走if语句（高度+）还是else语句（高度-）\r\n再到点击按钮\r\n点击按钮后再触发定时器\r\n高度变化至设定值时关闭定时器，及改变布尔值\r\n还有个问题就是因为navbar定位之后失去了撑开下方区块的功能，最后只能将父级nav的高度同步增大，不知道是不是有点绕了\r\n\r\n上代码：\r\n\r\n<!DOCTYPE html>\r\n<html lang="en">\r\n<head>\r\n    <meta charset="UTF-8">\r\n    <title>自适应导航栏</title>\r\n    <style>\r\n        /*简单清除默认样式*/\r\n        * {\r\n            margin: 0;\r\n            padding: 0;\r\n        }\r\n\r\n        /*导航宽屏时主体部分*/\r\n        nav {\r\n            width: 100%;\r\n            height: 50px;\r\n            background-color: #444;\r\n            overflow: hidden;\r\n            min-width: 320px;\r\n        }\r\n        .nav {\r\n            max-width: 80%;\r\n            margin: 0 auto;\r\n            height: 50px;\r\n        }\r\n        .logo {\r\n            width: 50px;\r\n            height: 50px;\r\n            float: left;\r\n        }\r\n        .logo img {\r\n            width: 100%;\r\n            display: block;\r\n        }\r\n\r\n        .navbar {\r\n            width: 80%;\r\n            height: 50px;\r\n            float: right;\r\n        }\r\n        .navbar ul {\r\n            list-style: none;\r\n            height: 50px;\r\n            overflow: hidden;\r\n            float: right;\r\n        }\r\n        .navbar li {\r\n            height: 50px;\r\n            float: left;\r\n            padding: 0 10px;\r\n        }\r\n        .navbar a {\r\n            display: block;\r\n            width: 100%;\r\n            height: 50px;\r\n            line-height: 50px;\r\n            text-decoration: none;\r\n            text-align: center;\r\n            color: #fff;\r\n            font-size: 16px;\r\n            font-weight: bold;\r\n        }\r\n\r\n        /*媒体查询屏幕宽度小于768px时生效以下效果*/\r\n        @media screen and (max-width: 768px) {\r\n            /*\r\n                1、navbar定位到nav下方，宽度撑满屏幕\r\n                2、高度给成0\r\n                3、超出部分不显示\r\n                4、其他样式相应调整\r\n            */\r\n            .navbar {\r\n                float: right;\r\n                position: absolute;\r\n                top: 50px;\r\n                left: 0;\r\n                width: 100%;\r\n                height: 0px;\r\n                background-color: #444;\r\n                overflow: hidden;\r\n            }\r\n            .navbar ul {\r\n                width: 100%;\r\n                height: 200px;\r\n            }\r\n            .navbar li {\r\n\r\n文／小pxu（简书作者）\r\n原文链接：http://www.jianshu.com/p/3a46ef5a95ab\r\n著作权归作者所有，转载请联系作者获得授权，并标注“简书作者”。', '2016-06-05'),
(9, '人工智能60年，后深度学习时代关键技术进展', '从1956年达特茅斯会议算起，人工智能发展到现在已是60周年。这一年，当初的参会者之一、被尊为“人工智能之父”的Marvin Minsky作古，而Google DeepMind开发的AlphaGo围棋人工智能系统以4:1的战绩挫败了人类顶级选手。\r\n\r\n回顾60年，人工智能发展阶段的划分可能见仁见智，但所有人都难以否认，人工智能经历过数次热潮与寒冬，而时下正值深度神经网络带来的“盛夏”——其支撑的语音识别、图像识别准确率已超过人类，这让从业者感到兴奋。要避免“寒冬”再次来临，我们需要正确理解技术的本质和价值，将最本质的变化应用于实际的开发。\r\n\r\n本期程序员封面报道，带来了以下实践分享：\r\n\r\n人工智能60周年，我们需要关注什么（周建丁）\r\nPeter Norvig：人工智能将是软件工程的重要部分（Peter Norvig，Google研究总监）\r\n语音识别系统及科大讯飞最新实践（魏思，张仕良，潘嘉等）\r\n使用深度学习打造智能聊天机器人（张俊林，中科院软件所博士）\r\n无人驾驶：人工智能三大应用造就“老司机”（韩威，宇通智能车队队长）\r\n知人知面需知心——论人工智能技术在推荐系统中的应用（洪亮劼，雅虎研究院高级研发经理）\r\n流动的推荐系统——兴趣Feed技术架构与实现（陈开江，边逛边聊联合创始人）\r\n资讯\r\n\r\nCSDN十大资讯 \r\nGoogle I/O 2016：人工智能真正的起点（宋锐，琥珀天气联合创始人兼COO）\r\n\r\n移动\r\n\r\n以架构和工具链优化Unity3D游戏开发流水线（樊松阳，梦城互动研发部经理）\r\n跨平台开发兴起，令使用Unity3D和Cocos2d-x开发移动游戏的团队日益增加。从目前的游戏市场看来，凭借过硬的品质来赢得用户，变得越来越主流。这对开发团队的技术提出了更高要求。本文以Unity3D为依托，探讨如何打造适合自己的游戏开发流水线。\r\n揭秘Android N新的编译工具JACK&JILL（李晓阳，美团客户端Android开发工程师）\r\n在Android 5.0修订版SDK中，上线了两款新编译器，名为JACK&JILL，它们象征着Google在进一步增强变异流程。JACK&JILL集成了混淆、资源裁剪、Multidex等原本以Gradle插件形势加入编译过程的工具，使用jayce作为IR，彻底脱离了对class字节码文件的依赖。本文将对JACK&JILL进行深度剖析，并对其使用进行讲解。\r\n当微软牛津计划遇到微信APP——微信实现部分（王豫翔，微软MVP）\r\n微软牛津计划（Project Oxford）提供了一系列机器学习API，包含计算机视觉、语音识别和语言理解等认知服务，它能为微信开发带来怎样有趣的功能？请看本文分解。\r\n如何编写基于编译时注解的Android项目（张鸿洋，CSDN博客专家，Android开发工程师）\r\n本文通过具体实例描述了如何编写一个基于编译时注解的项目，归纳起来，主要步骤为：项目结构的划分、注解模块的实现、注解处理器的编写和对外公布的API模块编写。通过文本的学习应该能够了解基于编译时注解这类框架运行的原理，以及自己如何编写这样一类框架。\r\n使用Unity开发HoloLens应用（张昌伟，全栈工程师，微软MVP。）\r\n开发者们在陆续收到HoloLens开发者版的同时，也都着手了HoloLens应用的开发工作。本文作者从空间映射、场景匹配、自然交互等核心特性开始，以实践详解了如何使用Unity引擎开发一个简单的HoloLens应用，并对自己的开发经验进行总结和分享。\r\n大数据\r\n\r\n高性能数据库中间件MyCAT（满春，宝龙集团DBA）\r\nMyCAT起源于Cobar，青出于蓝而胜于蓝，除了持续增强原来优异的分布式分库分表功能外，又结合集群管理、自动扩容、智能优化，成为互联网高性能中间件的代表。\r\nOpenStack能复制Red Hat的成功吗？（刘世民，云架构师）\r\n本文将讨论OpenStack的商业模式，并以Mirantis为例，与Red Hat的商业模式做对比。\r\n容器的性能监控和日志管理（林帆，ThoughtWorks公司DevOps技术咨询师）\r\n在运维基础设施中，服务状态和性能的监控预警是降低潜在故障风险最有效的措施，而对运行日志进行汇总收集则是在意外事故发生后快速定位问题、回溯故障现场的可靠手段。这次我们就来聊一聊在容器技术架构中实施性能监控和日志管理的话题。\r\n技术\r\n\r\n在调试器里看百度云管家（张银奎，《软件调试》和《格蠹汇编》作者）\r\n多少次，我打开任务管理器，看它忙碌的身影。多少次，我想大声对它说：“管家大哥，你歇歇，告诉我你在忙啥？”\r\n先进的银行反欺诈架构设计（杨税令，传世金融CTO）\r\n在互联网金融领域井喷式发展的同时，安全问题也日益严重，本文作者以银行为例，全面解读了反欺诈的架构设计。\r\n知识库专栏\r\n\r\nPHP学习指南（韩天峰，Swoole开源项目创始人） \r\nPHP并发I/O编程之路（韩天峰，Swoole开源项目创始人） \r\nPHP知识库内容精选\r\n\r\n百味\r\n\r\n漫画——为什么你需要一只猫（西乔）', '2016-06-05'),
(10, '微软向小米出售1500项专利，建立长期合作伙伴关系', '登录 | 注册\r\n\r\n发布链接发布图文\r\n  \r\n全部主题\r\n热门主题\r\n\r\n贡献榜全部\r\n\r\n钱曙光\r\n关注\r\nCSDN &《程序员》编辑/记者，我的邮箱qianshg@csdn.net\r\n\r\n\r\nRoyal_lr\r\n关注\r\n全栈攻城狮不知道是怎么炼成的\r\n\r\n\r\n过往记忆\r\n关注\r\n个人大数据技术博客：http://www.iteblog.com\r\n\r\n\r\nw_peace\r\n关注\r\n人生得意须尽欢，莫使金樽空对月。\r\n\r\n\r\n漫步云端的Cupid\r\n关注\r\n2\r\n微软向小米出售1500项专利，建立长期合作伙伴关系\r\n微软 小米 专利 阅读4402 \r\n据外媒报道，微软正在向中国设备制造商小米出售大约1500项专利。这一举动对一家美国公司来说非常罕见。两家公司声称，这是建立长期合作伙伴关系的开端。\r\n\r\n日前发布的交易中还包括了交叉授权协议，小米承诺在其手机和平板电脑上安装包括Office和Skype在内的微软程序。\r\n\r\n两家公司都拒绝透露交易的商务细节。\r\n\r\n交易前夕，小米副总裁王翔在电话中说道：“这是两家公司的一项重大合作协议”。\r\n\r\n有分析家评论，出于对专利弱保护期和旷日持久的法律纠纷的恐惧，小米拓展中国以外的市场一直受到阻碍，希望通过此次交易在海外市场占据一席之地。\r\n\r\n“这次交易可能会为小米带来大量专利以进军西方市场”，一位英国分析师Sameer Singh说道，“小米目前在中国受到了来自低端安卓供应商的持续冲击，现在必须进军海外市场”。\r\n\r\n据美国市场研究公司策略分析公司（Strategy Analytics）指出，小米手机在中国一季度的出货量同比下跌了9%，市场占有率从13%下降到12%。其市场份额不仅受到华为和三星电子的挤压，还受到了来自包括Oppo和Vivo在内的厂家竞争。\r\n\r\n据王翔透露，在去年小米在中国申请的3700项专利之上，这次获得的微软专利还包括语音通讯、多媒体和云计算，此举是“小米国际化迈出的重要一步”。\r\n\r\n这个月早些时候（5月19日），小米在美国发布了第一个电视机顶盒产品。该产品由小米与Alphabet有限公司下的Google合作开发。后者拥有安卓操作系统，小米的大多数设备在该系统上运行。小米还发布了一款运行微软Window系统的平板电脑。\r\n\r\n微软副总裁Jonathan Tinter说，通过在小米产品上预装自己的产品，微软渴望接近年轻、富裕且受过良好教育的小米用户。他拒绝透露有关专利交易的细节，但是全面的协议“会享受和其他战略合作伙伴一样的待遇”。\r\n\r\n一直以来，微软在持续减少与安卓设备制造商的许可交易，中国的制造商也不例外。\r\n\r\n曾经为微软提供咨询服务的专利专家Florian Mueller说：“微软实际进行专利出售的情况非常罕见”。他还补充说：“可能的情况是，微软发现对小米征收安卓专利税更加容易，专利转移只是更大交易的一部分”。\r\n\r\n原文：Microsoft sells patents to Xiaomi, builds ‘long-term partnership’ \r\n作者： Jeremy Wagstaff \r\n译者：赖信涛 \r\n责编：钱曙光\r\n \r\n钱曙光\r\n发布于GEEKNEWS 2016-06-03 09:09 分享到：  \r\n评论\r\n已有5条评论\r\n最新\r\n \r\nPsychiatric_patients 21小时前\r\n换种营销手段,小米还是那个无人可比的小米.毕竟爱的是MIUI.\r\n-1\r\n\r\nqq_35211731 2016-06-03 18:29\r\n支持小米，希望小米越来越强大\r\n0\r\n\r\nc牧殇c 2016-06-03 18:04\r\n年轻、富裕且受过良好教育的小米用户？？\r\n0\r\n\r\nliuwenchao888 2016-06-03 10:44\r\n小米会越来越强\r\n0\r\n\r\njjzaihaozhe 2016-06-03 09:59\r\n都在努力扩大自己的市场\r\n0\r\n添加评论\r\n发布链接\r\n发布图文\r\n返回顶部\r\n     发布到 主题  发布 评论', '2016-06-05'),
(11, '阿里云Apsara Stack：把万台集群技术塞进自有数据中心', '登录 | 注册\r\n\r\n发布链接发布图文\r\n  \r\n全部主题\r\n热门主题\r\n\r\n贡献榜全部\r\n\r\n钱曙光\r\n关注\r\nCSDN &《程序员》编辑/记者，我的邮箱qianshg@csdn.net\r\n\r\n\r\nRoyal_lr\r\n关注\r\n全栈攻城狮不知道是怎么炼成的\r\n\r\n\r\n过往记忆\r\n关注\r\n个人大数据技术博客：http://www.iteblog.com\r\n\r\n\r\nw_peace\r\n关注\r\n人生得意须尽欢，莫使金樽空对月。\r\n\r\n\r\n漫步云端的Cupid\r\n关注\r\n4\r\n阿里云Apsara Stack：把万台集群技术塞进自有数据中心\r\n专有云 阿里云 Apsara-Stack 混合云 阅读2676 \r\n当前探索企业云计算的技术工作者是幸运的，也是苦逼的。幸运的是，有阿里云、AWS、Azure等国内外公共云服务以及OpenStack、VMware等内部部署技术可以选择；苦逼的是，出于各种原因，完全的云化并不现实，而私有云技术又为目前的互通与将来的迁移造成新的门槛。有没有将二者结合的好办法呢？阿里云的答案是：有的，就是Apsara Stack专有云。\r\n\r\n互联网+背景下的上云挑战\r\n云计算社区的热闹和不断增长的云计算市场表明了政企组织机构对云计算的热情，IDC在去年11月份发布的报告称，在未来3年内大中型企业将加速向云计算转型。“互联网+”可以说是企业级云计算的核心战略目标——通过云的架构，企业可以获得弹性扩展的能力，以最具成本优势的模式支撑更多的用户，同时也可以实现应用的快速开发和部署支持业务的快速创新。\r\n\r\n然而，当前云计算在整个IT市场规模中的占比仍然只是很小的部分——根据工信部的数据，2015年中国云计算产业规模约为1500亿元，然而工信部2015年中国软件业务年收入百强的榜首收入规模就达到1482亿元。工信部另外一项数据，是我国云计算市场占全球云计算市场的份额仍不到5%。\r\n\r\n造成这种反差，一个重要的因素是企业用户尤其是传统行业的企业用户在实施云架构改造走向“互联网+”时面临重重挑战。\r\n\r\n政企机构出于自身数据中心利旧、数据本地化、安全合规以及其他各种原因的考量，现阶段不允许把所有业务系统全部放在公共云上。\r\n一些传统IT解决方案经过不同程度的改造之后以云计算的名义兜售，但实际上缺乏云的特性上，如多租户隔离、弹性扩展、高效运维监控的能力等。\r\n架构云化是一项复杂的任务，当前的各种混合云架构，在传统架构云化与公共云架构之间妥协，很难达到云的最佳效益，同时也为未来观念更新之后把系统搬上公共云留下新的坑。\r\n阿里云完成专有云拼版\r\n解决上述挑战，无疑需要一个部署在企业数据中心内部的云计算，这个云计算首先是与外界隔离的，保障了安全合规的需求，但与一般的私有云不同，它需要具有满足互联网业务爆发式增长的弹性扩展的能力，也要具有高稳定、自动化运维等云架构的特性，最好还能降低日后迁移到公共云平台的技术门槛，可以视为一个缩略版的公共云，也就是“专有云”。作为一家老牌云计算服务提供商，阿里云走出了这一步。\r\n\r\n在深圳云栖大会上，阿里云推出了专有云（Apsara Stack）解决方案，支持企业客户在自己的数据中心内部署完整的云计算服务。Apsara Stack提供了包括云计算、大数据、企业级互联网架构、安全等全栈云产品的API和SDK，满足了客户定制化要求和生态建设。企业可以将Apsara Stack部署在自有的数据中心环境，实现完全隔离的环境下的自主管理运维。\r\n\r\n图片描述\r\n\r\n据阿里云研发总监介绍，客户可通过调用API，对产品、资源、应用和数据进行统一入口管理，实现灵活部署、快速操作、精确使用和及时监控。此外，客户还可以自己封装个性化的管理系统，与企业现有流程无缝对接；生态合作伙伴也可以利用开放接口进行二次封装，实现行业化定制管理方案。\r\n\r\n推出Apsara Stack之后，阿里云自身提供的云计算服务覆盖了公共云、混合云、专有云三大领域。与此同时，阿里云也与其他云计算服务商一起实现多种架构的混合云形态，充分满足不同客户群体对计算的需求。\r\n\r\nApsara Stack架构特点\r\n从Apsara（飞天）为阿里云自2009年开始研发的云计算服务的代号，容易知道这个Stack和OpenStack在底层技术上是不同的。Apsara Stack和OpenStack以及市面上流行的各种“私有云”主要区别如下：\r\n\r\nApsara Stack脱胎于阿里云“飞天”（Apsara）技术体系，和公共云同源同根，经受了长期验证，支持更大规模，具有更广泛的产品线；\r\n私有云市场现状鱼龙混杂，各种一体机、虚拟化、硬件设备都借用了云的概念，而Apsara Stack是真正的平台，能够帮助客户构建云计算、大数据、安全的企业级平台。\r\nApsara Stack底层与公共云一致，意味着获得无限的升级可能；政企用户可以享受一致性体验的“专有云+公共云”的混合云服务，既能为本地数据中心赋予阿里云同款云架构能力，又能无缝获取公共云的弹性扩展能力，无需考虑软件架构的差异。\r\n\r\n阿里云技术专家介绍，Apsara Stack具备IaaS、数据库、大数据、安全、中间件等全套优势，适用于客户集群规模在50台-1万台。相比之下，OpenStack侧重在500台以下的小规模场景下的IaaS层面，也缺乏一揽子的云产品。\r\n\r\n企业级服务的保证\r\n“互联网+”应用的开发，不考虑海量用户的支持是不合格的，但只考虑规模、并发和稳定也是不够的。中国应用的两个特点，首先是根本的企业级服务，二是应用的边界会随着业务的发展而扩张。后者要求云平台所支持服务具有足够的丰富性来保证灵活应用开发的扩展，包括大数据、物联网以及各种行业解决方案的支持；前者则要求这种丰富性要涵盖到传统企业软件，对于做“互联网+”的传统行业来说，对SAP、Oracle老牌企业级应用的支持是不可或缺的。\r\n\r\n在推出Apsara Stack的同时，阿里云也构建了一个生态系统，来满足不同政企机构对不同应用的需求。除了将阿里巴巴内部的大数据、人工智能技术开放，以及推出物联网、视频云解决方案，阿里云也和行业合作伙伴打造行业解决方案，比如某医院选择阿里云的原因之一就是其医疗方案的成熟度。\r\n\r\n其次，阿里云与传统IT服务商的合作也获得了进步，继兼容Oracle数据库之后，阿里云还和SAP达成合作，使得企业可直接在阿里云上使用SAP HANA数据库、ERP/CRM等商务应用解决方案和SaaS化软件产品。\r\n\r\n这种合作也涵盖了运维层面，在发布Apsara Stack时，阿里云资深总监李津提到，某合作伙伴的云管控中心是Apsara Stack中的重要组成部分，可以提供基础云资源的运维，以及基于专有云对外提供运营服务的云自助门户的功能。\r\n\r\n除了自身的服务，阿里云云市场上还提供了包括用友、宏碁资讯、润和软件、驻云、空桥克拉、曙安数据、叠云、泛微、畅捷通、通达等近500家海内外知名软件商的逾3000款软件。\r\n\r\n结语\r\n公共云才是真正的云计算，这是阿里云一贯的观点。但在服务客户的实践中，阿里云也意识到，当前的IT应用水平距离实现完全的公共云还有一段遥远的路程。Apsara Stack的推出，为这段路架起了一座桥梁，完善了阿里云的战略版图，也成为了海关总署、浙江政务服务网、网商银行、贵州交警等大型政企机构构建新IT平台的基石。阿里云表示，作为阿里云公共云的一个副产品，Apsara Stack是阿里云的战略选择，是一个极富价值但成本很低的项目，一定会持续投入。\r\n \r\nCSDN周建丁\r\n发布于云计算 2016-06-03 17:18 分享到：  \r\n评论\r\n已有2条评论\r\n最新\r\n \r\nqq_34608710 2016-06-04 14:18\r\n这么厉害，张见识了!\r\n0\r\n\r\n丁国华 2016-06-04 06:55 来自 移动客户端\r\n涨知识了！\r\n0\r\n添加评论\r\n发布链接\r\n发布图文\r\n返回顶部\r\n     发布到 主题  发布 评论', '2016-06-05'),
(12, 'Apache Spark 2.0前瞻：为机器学习模型注入持久性', '登录 | 注册\r\n\r\n发布链接发布图文\r\n  \r\n全部主题\r\n热门主题\r\n\r\n贡献榜全部\r\n\r\n钱曙光\r\n关注\r\nCSDN &《程序员》编辑/记者，我的邮箱qianshg@csdn.net\r\n\r\n\r\nRoyal_lr\r\n关注\r\n全栈攻城狮不知道是怎么炼成的\r\n\r\n\r\n过往记忆\r\n关注\r\n个人大数据技术博客：http://www.iteblog.com\r\n\r\n\r\nw_peace\r\n关注\r\n人生得意须尽欢，莫使金樽空对月。\r\n\r\n\r\n漫步云端的Cupid\r\n关注\r\n0\r\nApache Spark 2.0前瞻：为机器学习模型注入持久性\r\n开源 大数据 Spark 机器学习 阅读5048 \r\n原文链接：Apache Spark 2.0 Preview: Machine Learning Model Persistence \r\n作者：Joseph Bradley \r\n译者：郭芮（guorui@csdn.net）\r\n简介\r\n\r\n研究机器学习用例：\r\n\r\n数据科学家建立了一个ML模型，并交给了一个工程团队在生产环境部署。\r\n数据工程师将使用Python的模型训练工作流和Java模型服务工作流整合。\r\n数据科学家专门设立岗位来训练后期需要被保存和评估的ML模型。\r\n在所有的这些例子中，如果有了模型的持久性，那么保存和加载模型的问题将变得更容易解决。在即将到来的2.0版本中，通过基于DataFrame的API，Spark机器学习库MLlib将实现几乎完整的ML持久性支持。本文将提前透露有关代码示例，以及MLlib API持久性的一些细节。\r\n\r\nML持久性的关键特性包括：\r\n\r\nSpark支持所有语言的API：Scala、Java、Python和R\r\n基于DataFram的API几乎支持所有的ML算法\r\n支持单一模型和完整的Pipelines，不管是训练或者未训练的\r\n使用可互换的格式来实现分布式存储\r\n感谢所有为MLlib带来巨大发展的社区贡献者们!在JIRAs中可以看到为Scala，Java， Python和R做出贡献的完整人员名单。\r\n\r\n了解API\r\n\r\n在Apache Spark 2.0里，对于MLlib来说基于DataFrame的API在关于Spark的ML中占据了首要位置。该API模仿被人们所熟知的Spark Data Source API，提供保存和加载模型的功能。\r\n\r\n下面将采用流行的MNIST数据集进行手写体数字识别，并在几种语言上演示保存和加载模型的功能（LeCun等著，1998；可从LIBSVM数据页面获取）。这个数据集包含了手写数字0-9，以及地面实况标签。这里有些例子：\r\n\r\n图片描述\r\n\r\n我们的最终目的是为了拍摄新的手写数字图像并进行数字识别。在下面的笔记中就完整地演示了数据载入，以及模型训练、保存和加载的代码。\r\n\r\n保存和加载单一模型\r\n\r\n首先将展示如何保存和加载单一模型以促进语言共享，例子中首先会通过Python来训练一个Random Forest Classifier并保存下来，然后再利用Scala加载相同的模型。\r\n\r\ntraining = sqlContext.read...  # data: features, label\r\nrf = RandomForestClassifier(numTrees=20)\r\nmodel = rf.fit(training)\r\n为了简化，这里将保存模型称为save方法，把加载模型称为load方法：\r\n\r\nmodel.save("myModelPath")\r\nsameModel = RandomForestClassificationModel.load("myModelPath")\r\n我们还可以将同样的模型（已保存在Python的）加载到一个Scala或者Java应用程序中：\r\n\r\n// Load the model in Scala\r\nval sameModel = RandomForestClassificationModel.load("myModelPath")\r\n这个方法既可以用于小型的本地模型例如K-Means模型（为了集群），也可以用于大型的分布式模型例如ALS模型（为了推荐）。因为加载的模型具有相同的参数设置和数据，所以即使加载的是一个完全不同的Spark部署，它也会给出相同的预测。\r\n\r\n保存和加载完整Pipelines\r\n\r\n到目前为止只演示了单一ML模型的保存和加载，但在实际过程中，ML的工作流其实包含着许多阶段，从特征的提取和转换到模型的训练和调优都在其中。MLlib还提供了Pipelines来帮助用户更好地构建这些工作流。\r\n\r\n同时，MLlib还允许用户保存和加载整个Pipelines。下面通过一个Pipeline案例看一下它是采用了哪些步骤实现的：\r\n\r\n特征提取：使用Binarizer将图像转换成黑白色\r\n模型训练：使用Random Forest Classifier拍摄图像和预测数字0–9\r\n调优：使用交叉验证（Cross-Validation）来优化森林中树的深度\r\n下面是建立Pipeline的一个片段：\r\n\r\n// Construct the Pipeline: Binarizer + Random Forest\r\nval pipeline = new Pipeline().setStages(Array(binarizer, rf))\r\n\r\n// Wrap the Pipeline in CrossValidator to do model tuning.\r\nval cv = new CrossValidator().setEstimator(pipeline) ...\r\n在管道训练之前，我们会演示将整个工作流保存下来的过程(训练前)。而且这个工作流可以在另一个数据集上，或者是在另一个Spark集群上等地方加载运行。\r\n\r\ncv.save("myCVPath")\r\nval sameCV = CrossValidator.load("myCVPath")\r\n最后，我们就可以进行Pipeline训练，再将其保存和加载。这不仅可以节省特征提取的步骤，还可以省去使用Cross-Validation调整Random Forest模型以及从模型调优中提取数据的过程。\r\n\r\nval cvModel = cv.fit(training)\r\ncvModel.save("myCVModelPath")\r\nval sameCVModel = CrossValidatorModel.load("myCVModelPath")\r\n了解细节\r\n\r\nPython调优\r\n\r\n很遗憾，Python调优将缺席Spark 2.0版本。就目前情况来看，Python还不支持保存和加载用于优化hyperparameters模型的CrossValidator和TrainValidationSplit；这个问题也正是Spark 2.1版本需要解决的。但是，它仍然有可能被用来保存Python中的CrossValidator和TrainValidationSplit结果。例如，使用Cross-Validation来调整Random Forest并将调整过程中发现的最好模型保存起来。\r\n\r\n# Define the workflow\r\nrf = RandomForestClassifier()\r\ncv = CrossValidator(estimator=rf, ...)\r\n# Fit the model, running Cross-Validation\r\ncvModel = cv.fit(trainingData)\r\n# Extract the results, i.e., the best Random Forest model\r\nbestModel = cvModel.bestModel\r\n# Save the RandomForest model\r\nbestModel.save("rfModelPath")\r\n可交换的存储格式\r\n\r\n在内部，我们可以把模型的元数据和参数保存为JSON，把数据保存为Parquet。这些存储格式是可交换的，还可以使用其他库读取。Parquet不仅可以存储小的模型（例如Naive Bayes for classification），还可以存储大型的分布式模型（例如ALS for recommendation）。任何被Dataset/DataFrame支持的URI 都可以保存和加载存储路径，包括S3路径、本地存储等等。\r\n\r\n语言的跨平台兼容性\r\n\r\n利用Scala、Java和Python可以很容易地保存和加载模型，但是R却有两个局限性。一方面，R并不是支持所有的MLlib模型，其他语言所训练的模型也不是都可以被加载到R。另一方面，目前的R模型格式需要存储一些配合R使用的数据，这样给其他语言加载R所训练和存储的模型增加了困难。相信更好的跨语言支持R会在不久的将来被补足。\r\n\r\n结论\r\n\r\n随着2.0版本的即将发布，DataFrame-based MLlib API将几乎完全覆盖持久化的模型和Pipelines。对于团队间共享模型、多语言ML工作流创建以及将模型用于生产这些，持久性发挥着至关重要的作用。这个特性也将会推动MLlib API（DataFrame-based）最终转变为Apache Spark机器学习的重要API。\r\n\r\n接下来呢?\r\n\r\n未来的话，更高优先级的项目将会包括完整的持久性覆盖、Python模型优化算法以及R和其他语言API之间的兼容性改进。\r\n \r\n郭芮\r\n发布于Spark 2016-06-02 17:30 分享到：  \r\n评论\r\n已有1条评论\r\n最新\r\n \r\nnicolusslee 2016-06-02 23:46\r\n6不错，很受用\r\n0\r\n添加评论\r\n发布链接\r\n发布图文\r\n返回顶部\r\n     发布到 主题  发布 评论', '2016-06-05'),
(13, '深度学习指南：基于Ubuntu从头开始搭建环境', '登录 | 注册\r\n\r\n发布链接发布图文\r\n  \r\n全部主题\r\n热门主题\r\n\r\n贡献榜全部\r\n\r\n钱曙光\r\n关注\r\nCSDN &《程序员》编辑/记者，我的邮箱qianshg@csdn.net\r\n\r\n\r\nRoyal_lr\r\n关注\r\n全栈攻城狮不知道是怎么炼成的\r\n\r\n\r\n过往记忆\r\n关注\r\n个人大数据技术博客：http://www.iteblog.com\r\n\r\n\r\nw_peace\r\n关注\r\n人生得意须尽欢，莫使金樽空对月。\r\n\r\n\r\n漫步云端的Cupid\r\n关注\r\n5\r\n深度学习指南：基于Ubuntu从头开始搭建环境\r\npython cuda 深度学习 cuDNN Tensorflow 阅读8536 \r\n原文链接：Setting up a Deep Learning Machine from Scratch (Software) \r\n译者：刘翔宇 审校：赵屹华 \r\n责编：周建丁（zhoujd@csdn.net）\r\n这是一篇为机器搭建深度学习研究环境的详细指南，包括驱动程序、工具和各种深度学习框架的安装指导。在64位Ubuntu 14.04的机器上使用Nvidia Titan X进行测试。\r\n\r\n还有一些有类似目的的指南。一些内容有限，而另外一些则不是最新的。该指南基于（有些部分是复制来的）：\r\n\r\n在Ubuntu上安装Caffe\r\n运行深度学习的梦想机器\r\n目录\r\n基础知识\r\nNvidia驱动\r\nCUDA\r\ncuDNN\r\nTensorflow\r\nOpenBLAS\r\n常用工具\r\nCaffe\r\nTheano\r\nKeras\r\nTorch\r\nX2Go\r\n基础知识\r\n首先，打开终端，运行以下命令确保你的操作系统是最新的\r\nsudo apt-get update  \r\nsudo apt-get upgrade  \r\nsudo apt-get install build-essential  \r\nsudo apt-get autoremove\r\n安装git\r\n\r\nsudo apt-get install git\r\nNvidia驱动\r\n查看显卡型号\r\nlspci | grep -i nvidia\r\n到Nvidia官网查找你显卡对应的最新驱动和系统设置。你可以从此网站上下载并安装驱动，但这样做会升级到更新的驱动，并且卸载的时候会有些麻烦。此外，这么做需要你退出X服务会话，从终端进行安装，这比较麻烦。\r\n\r\n我们将使用apt-get来安装驱动。到 “Proprietary GPU Drivers” PPA中查看是否有你最新的驱动。注意，最新的驱动一定是最稳定的。你也可以安装网页上推荐的驱动版本。添加”Proprietary GPU Drivers” PPA 资源库。在写这篇文章的时候，最新版本是361.42，然而推荐版本是352：\r\n\r\nsudo add-apt-repository ppa:graphics-drivers/ppa\r\nsudo apt-get update\r\nsudo apt-get install nvidia-352\r\n重启系统\r\nsudo shutdown -r now\r\n检查以确保安装了正确版本的NVIDIA驱动\r\ncat /proc/driver/nvidia/version\r\nCUDA\r\n从Nvidia上下载CUDA7.5。然后到下载目录下安装CUDA\r\nsudo dpkg -i cuda-repo-ubuntu1404*amd64.deb\r\nsudo apt-get update\r\nsudo apt-get install cuda\r\n添加CUDA到环境变量\r\necho ''export PATH=/usr/local/cuda/bin:$PATH'' >> ~/.bashrc\r\necho ''export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH'' >> ~/.bashrc\r\nsource ~/.bashrc\r\n检查以确保安装了正确版本的CUDA\r\nnvcc -V\r\n重启系统\r\nsudo shutdown -r now\r\n检查CUDA安装（可选）\r\n在CUDA安装目录安装样例。编译它们（需要几分钟）：\r\n/usr/local/cuda/bin/cuda-install-samples-7.5.sh ~/cuda-samples\r\ncd ~/cuda-samples/NVIDIA*Samples\r\nmake -j $(($(nproc) + 1))\r\n注意：（-j $(($(nproc) + 1))）命令使用你机器上的核心数并行执行，所以编译会更快\r\n\r\n运行deviceQuery，确保它能检测到显卡并测试通过\r\nbin/x86_64/linux/release/deviceQuery\r\ncuDNN\r\ncuDNN是为DNN设计的CPU加速库。它能在多种情况下帮助提升执行速度。为了下载cuDNN库，你需要到Nvidia网站https://developer.nvidia.com/cudnn上进行注册。几小时到几个工作日就能够批准。一旦注册批准，下载Linux版本的cuDNN v4。最新版本是cuDNN v5，但是不是所有的工具都支持。\r\n\r\n解压并复制文件\r\n\r\ncd ~/Downloads/\r\ntar xvf cudnn*.tgz\r\ncd cuda\r\nsudo cp */*.h /usr/local/cuda/include/\r\nsudo cp */libcudnn* /usr/local/cuda/lib64/\r\nsudo chmod a+r /usr/local/cuda/lib64/libcudnn*\r\n检查\r\n你可以使用nvidia-smi 命令检查目前所有操作都正确。这应该会输出GPU的一些统计数据\r\nTensorflow\r\n安装v0.8版本与GPU兼容。下面的指令都来自于这里\r\nsudo apt-get install python-pip python-dev sudo pip install --upgrade \r\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl\r\n运行一个测试程序确保Tensorflow成功安装。当你执行import命令的时候，应该不会有警告/错误。\r\npython\r\n>>> import tensorflow as tf\r\n>>> exit()\r\nOpenBLAS\r\nOpenBLAS是一个线性代数库，比Atlas更快。这一步是可选的，但要注意，下面的一些步骤假定你已经安装了OpenBLAS。你需要安装gfortran来编译它。\r\nmkdir ~/git\r\ncd ~/git\r\ngit clone https://github.com/xianyi/OpenBLAS.git\r\ncd OpenBLAS\r\nmake FC=gfortran -j $(($(nproc) + 1))\r\nsudo make PREFIX=/usr/local install\r\n将路径添加到LD_LIBRARY_PATH 变量中\r\necho ''export LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH'' >> ~/.bashrc\r\n常用工具\r\n为Scipy安装一些常用工具\r\nsudo apt-get install -y libfreetype6-dev libpng12-dev\r\npip install -U matplotlib ipython[all] jupyter pandas scikit-image\r\nCaffe\r\n下面的指令都来自于这里。第一步是安装所必须的文件\r\nsudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compiler\r\nsudo apt-get install --no-install-recommends libboost-all-dev\r\nsudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev\r\n克隆Caffe资源库\r\ncd ~/git\r\ngit clone https://github.com/BVLC/caffe.git\r\ncd caffe\r\ncp Makefile.config.example Makefile.config\r\n如果你安装了cuDNN，取消Makefile中USE_CUDNN := 1 这一行的注释\r\nsed -i ''s/# USE_CUDNN := 1/USE_CUDNN := 1/'' Makefile.config\r\n如果你安装了OpenBLAS，修改BLAS参数值为open\r\nsed -i ''s/BLAS := atlas/BLAS := open/'' Makefile.config\r\n安装需要的文件，构建Caffe和测试，运行测试确保所有测试都通过。注意，这都需要一段时间。\r\nsudo pip install -r python/requirements.txt\r\nmake all -j $(($(nproc) + 1))\r\nmake test -j $(($(nproc) + 1))\r\nmake runtest -j $(($(nproc) + 1))\r\n构建PyCaffe，Caffe的Python接口\r\nmake pycaffe -j $(($(nproc) + 1))\r\n将Caffe添加到环境变量中\r\necho ''export CAFFE_ROOT=$(pwd)'' >> ~/.bashrc\r\necho ''export PYTHONPATH=$CAFFE_ROOT/python:$PYTHONPATH'' >> ~/.bashrc\r\nsource ~/.bashrc\r\n测试确保Caffe安装成功。当执行import命令的时候应该不会有警告/错误。\r\nipython\r\n>>> import caffe\r\n>>> exit()\r\nTheano\r\n安装所必须的文件，然后安装Theano。这些指令来自于这里\r\nsudo apt-get install python-numpy python-scipy python-dev python-pip python-nose g++ python-pygments python-sphinx python-nose\r\nsudo pip install Theano\r\n测试Theano安装。当执行import命令的时候应该不会有警告/错误。\r\npython\r\n>>> import theano\r\n>>> exit()\r\nKeras\r\nKeras是围绕Theano和Tensorflow设计的一个有用的封装。默认情况下，它使用Theano作为后端。查看这里的指令以了解如何变更为Tensorflow。\r\nsudo pip install keras\r\nTorch\r\n下面安装Torch的指令来自于这里。安装会花一些时间\r\ngit clone https://github.com/torch/distro.git ~/git/torch --recursive\r\ncd torch; bash install-deps;\r\n./install.sh\r\nX2Go\r\n如果你的深度学习机器不是主要工作机器，X2Go可以帮助你远程访问。X2Go是一个了不起的远程访问解决方案。你可以使用下面的命令在Ubuntu机器上安装X2Go服务。\r\nsudo apt-get install software-properties-common\r\nsudo add-apt-repository ppa:x2go/stable\r\nsudo apt-get update\r\nsudo apt-get install x2goserver x2goserver-xsession\r\nX2Go不支持统一桌面环境（Ubuntu的默认环境）。我发现XFCE效果不错。更多支持的环境在这里\r\nsudo apt-get update\r\nsudo apt-get install -y xfce4 xfce4-goodies xubuntu-desktop\r\n使用下面的命令查看机器的IP\r\nhostname -I\r\n你可以使用上面的IP在你主要使用的机器上安装一个客户端来连接到深度学习服务器上。根据你的客户端系统，这里有更多的指令。\r\n \r\nCSDN周建丁\r\n发布于人工智能 2016-06-01 09:40 分享到：  \r\n评论\r\n已有8条评论\r\n最新\r\n \r\nHH_Lab 2016-06-03 09:58\r\n居然还到处转载，请问大牛自己实践过吗？你装Nvida驱动的时候不卸载原来的驱动，重启进去后会造成驱动加载冲突，登录界面循环。有的新手连ubantu的root密码都没来得及改。请自己在客户机试过后再来发表好吗。\r\n0\r\n\r\nxiaozhuzhuzpp 2016-06-03 08:32\r\n讲得很详细，受教了\r\n0\r\n\r\ngotope 2016-06-02 13:17\r\n第一步 Nvidia驱动 然后又CUDA安装, \r\nCUDA不是包含了N驱动? \r\n一开始就应该CUDA安装\r\n0\r\n\r\n花开莫与流年错_ 2016-06-02 08:39\r\n讲得很详细，学习了\r\n0\r\n\r\nzcy黑马 2016-06-01 15:50\r\n讲的蛮详细，应有尽有\r\n0\r\n\r\nbokee 2016-06-01 15:12\r\n不错不错不错\r\n0\r\n\r\nqq_34986333 2016-06-01 14:34\r\n66666666666\r\n0\r\n\r\nlq200015 2016-06-01 11:33\r\n不错学习了\r\n0\r\n添加评论\r\n发布链接\r\n发布图文\r\n返回顶部\r\n     发布到 主题  发布 评论', '2016-06-05'),
(14, '数据立法：你的隐私谁来保护？', '数据分析网\r\n大数据 \r\n资讯+观点+技术研究中心\r\n首页\r\n大数据新闻\r\n人物观点\r\n大数据\r\n数据分析\r\n软件工具\r\n活动沙龙\r\n投稿\r\n大数据导航\r\n大数据资料\r\n数据分析视频\r\n数据分析图书\r\n热门主题\r\n作者列表\r\n关于我们\r\n关注本站 \r\nHi, 请登录     我要注册     找回密码\r\n\r\n数据立法：你的隐私谁来保护？\r\n2016-06-02 分类：人物观点 评论(0) \r\n导读：数据时代更加强调数据的公开，政府能把自己的掌握数据向社会开放，创造更多的经济价值。从数据来讲强调数据流动，因为数据只有流动才能把价值发挥到最大化。对于涉及到个人数据，涉及到个人隐私的时候要强调保护，涉及到不同数据的时候，立法应该有不同的侧重点。\r\n\r\n从立法的角度来看，实际有两个视角，一个是发展，一个是安全。需要立法部门研究，也需要实务部门提出可行性的对策。如何平衡这两个维度既有重要的机遇，对行业发展也具有重大的价值。\r\n\r\n我今天报告的题目是“数据立法的进展及对数据经济的影响”。 在座从事法律专家不是那么多，我后面对立法所涉及的一些基本概念、基本原则做一个铺垫介绍。第二个部分围绕迄今为止和数据相关的主要立法情况，针对性做一些数据经济关联的介绍。第三个部分结合马上要出台一些的法律或者正在起草的法律，看未来会对我们数据产生那些影响。\r\n\r\n一、基本概念\r\n\r\n首先中国法律的位阶。从宪法到规章都是法律的正式渊源。第二层的法律是狭义的法律，指全国人大和常委会通过的法律文件。这个表表示在不同法律文件之间的位阶关系。下位阶的文件不能抵触上位阶文件。\r\n\r\n\r\n\r\n下面这个表强调了不同的法律位阶文件之间的调整范围关系。特别是看到地方性法规和行政法规都可以对法律未作规定的部分事项先行立法。这是因为立法机关面临经济社会发展中出现的问题，回应的节奏是相对滞后的。\r\n\r\n\r\n\r\n在这种情况下，我们的《宪法》是赋予国务院和地方人大及其常委会针对一些法律未规定的事项可以先行立法：国务院可以职权范围管理做出规定；地方人大是可以根据本地方特点做出一些试验性的探索。下面介绍的时候会介绍近期贵州省出台的一个地方性法规，事实上它就属于具有创新性、试点性的法律。这个法规事实上就是在目前狭义的法律上没有规定的情况下，鼓励地方立法创新，去面临数据时代的时代变革。\r\n\r\n规章也是有两个不同的维度，一个是国务院各个部门，比如发改委、工商总局、商务部这些部门制定的规章。二个是地方政府出台的规章，他们的上位法依据可以是国务院制定的行政法规，也可以是本地方的人大所制定的地方性法规。地方性的政府规章虽然规定适用的范围仅限于每个地方，但是上位法的渊源更广泛。\r\n\r\n从一般法理来看，基本原则包括：\r\n\r\n 法治原则。为什么在这里提到？作为数据经济的角度来说肯定会涉及到财产权的问题。如果涉及到政府向企业调用数据，有可能涉及到这里面提到征收和征用的问题。数据发展过程中也会涉及到隐私权问题。隐私权从法律角度来讲实际是涉及到人格权的问题，对这些内容规定都涉及到法治原则。我们公权力机关事实上有很强烈的立法需求，不断提高自己制定的法律文件的位阶，去增加管理手段。\r\n\r\n比例原则。第一目的要有正当性，出台这个法律一定是要有合法、正当且必要的目的，或者为了公共安全或者为了经济发展。为了实现这个目的所采取的手段必须是成本最低的，不能给社会带来更大的负担和成本。如果这个手段带来利益的同时也会影响另一方利益，就要做出平衡，确保最后所造成的损害是比较小的。从目的的角度来解读，数据时代更加强调政府数据的公开，政府能把自己的掌握数据向社会开放，创造更多的经济价值。从商业数据来讲强调数据流动，因为数据只有流动才能把价值发挥到最大化。对于涉及到个人数据，涉及到个人隐私的时候要强调保护。涉及到不同数据的时候，立法应该有不同的侧重点。\r\n\r\n公开原则。中国立法公开为什么做得这么好？其实从国际推动国内的改革，特别是加入WTO之后，中国通过的对贸易可能造成影响的法规都要向WTO其他成员国通报，通报事实上就是一个公开的过程。\r\n\r\n二、数据立法的情况\r\n\r\n法律层面，最早和数据相关的立法可能就是全国人大常委会在2000年通过的一个文件。在立法里面有些概念使用不太清晰，有用信息、有用数字、也有用电子化，我觉得应当对这些概念做进一步梳理，明确使用范围，以对我们立法有一个更好的指引。前期的立法有很多是部门推动，不同部门从不同利益诉求出发，概念就会有不同的冲突和碰撞。\r\n\r\n2000年全国人大常委会通过的决定，主要从互联网的运行安全和信息安全方面强化刑事责任规定。2004年通过电子签名法，电子签名法现在讨论研究比较少。\r\n\r\n2012年全国人大常委会通过的决定，和刚才提到的2000年决定，是12年会同一个月份同一天通过的。这个决定是关于加强网络信息保护的决定。这个可以看到里面调整的主要内容是加强网络信息保护，主要识别公民信息和公民隐私的电子信息。\r\n\r\n这句话可以分两部分：一个是说能够识别个人公民身份的电子信息；二个是涉及公民隐私的个人信息。我们在商务活动中，针对这种不同类型的信息是有不同的要求，涉及到公民个人身份的信息，它必须有一个前提条件是能够识别，具有可识别性，才属于法律所保护的范围。\r\n\r\n如果从身份化的手段去处理就能处理这个要求，但对于个人隐私是绝对不需要保护，不需要加入可识别性。如果在逻辑上明确个人隐私的话，基本上已经具体到个人。\r\n\r\n关于保护原则：这个决定基本上从欧盟到OECD，再到美国提出个人信息保护原则，基本上都是一致的。首先应当遵循这样一个合法、正当、必要原则，明确其使用信息和范围，就是一个告知义务。并且要经收集者同意。\r\n\r\n决定提到不能违反法律、法规的规定。\r\n\r\n这里的法规规定就包括两种，既包括国务院行政法规，也包括地方人大常委会通过行政法规。如果在某个地方从事数据的交易或者服务活动，你一定要注意，当地地方人大有没有制定这样一个文件？如果地方人大有通过这方面的法律，有具体的要求，你在遵循国家行政法规同意的要求同时，还要恪守地方人大的行政法规。\r\n\r\n最后，网络提供者和其他企业事业单位收集电子信息应该公开其收集使用规则。这个法律还有一个比较重要的意义，它实际上在法律层面上面，正式把实名制给确认起来。要求网络提供者为用户办理服务的，应当与用户签定协议或者确认提供服务时要求用户提供真实身份信息。我想实名制管理一方面作为国家宏观的，作为秩序的规管，作为秩序的维护所要求的。另一方面，对有一些经济活动也是必要的，后面也会提到一些金融行业领域，实名制是可以更加重要的保证。虽然这个决定内容比较短，没有一个章节的划分，只有十几条内容，但是基本上框定了我们中国关于网络信息保护一些基本制度。\r\n\r\n这里还必须提到刑法。刑法就跟经济数据或者跟个人信息相关，就是2009年通过的刑法修正案七和去年通过的刑法修正案九，这里有一个侵犯公民信息罪，从这个可以看到对公民信息的保护力度更强了，对公民信息处理要求更严了，在涉及到公民信息交易活动的时候，要特别注意合规方面的一些要求，刑责也上升了。\r\n\r\n《消费者权益保护法》这个法律从1993年通过到2013年有一个比较大规模的修整，它的核心内容和我们相关的，将个人信息受到保护明确作为消费者一项基本权利。如果我们提供这个数据交易服务是面向普通网民就要特别注意。消费者在购买使用商品和接受服务时，前面已经有的，这里提到享有个人信息依法受到保护的权利，其中有关个人数据利用规则跟2012年的决定要求是基本一样的。\r\n\r\n《国家安全法》是去年通过的。一个是信息安全行业利好。国家在这方面加强能力建设，肯定在相关网络、相关技术、研究应用方面要有投入。国家在《国家安全法》里面提出这样一个目标，实现核心技术、关键技术设施和重要领域信息技术和数据安全的可控。如果在这个行业做一些重要的投入，我想在未来会有更好的发展前景。\r\n\r\n另外需要注意的是，国家现在已经确定这样一个原则，要建立国家安全审查和监督机制，对影响或者可能影响国家安全的外商投资网络信息技术进行国家安全审查，现在正在起草行政法规。一旦行政法规出台有具体制度的，后面就会有更明确的要求。\r\n\r\n下面简单讲下电信条例法规方面的要求：\r\n\r\n电信条例有两个亮点：第一支持民营资本进入电信行业，第二对互联网数据传送业务进行细分。如果对新型类型数据服务申请的是新型许可证。\r\n\r\n《政府信息公开条例》2008年通过。政府公开信息条件涉及到两方面，一个是政府主动公开发布一些信息，二个是依申请公开流程，公民、法人和其他组织可以根据生产、生活、科研需要向政府申请公开申请。\r\n\r\n《征信业管理条例》是2012年通过的，一定要遵循这个条例里所规定的内容，特别是针对个人信息的采集和限制有一些具体的要求。基因、信仰这些敏感信息是禁止采集的。\r\n\r\n《计算机信息系统安全保护条例》和《国家联网的管理办法》。\r\n\r\n《贵州省大数据发展应用促进条例》通过地方性法规，有一些创新。\r\n\r\n三、立法趋势及其影响\r\n\r\n\r\n\r\n关于大数据加强市场主体服务和监管，就是大数据如何监管，中国将来推出一系列数据监管平台，我想这个监管平台建设出来之后，监管数据也是后面进一步挖掘，能够怎么样发挥更好的经济价值一个载体。\r\n\r\n立法规划：每一个文件分三个档，全国人大常委会分三类，国务院分三类，可以看到轻重缓急程度不同。\r\n\r\n最后想强调一点，我们在立法过程中，实际有两个主题，一个是发展，一个是安全。需要立法部门研究，也需要实务部门提出可行性的对策。对个人信息保护，可以从我们取信于消费者的视角来理解。对应发展和保护，我们可以从电子商务法和网络安全法来评估。我想未来的立法能够从这两个法律具体条文规范内容看要求。如何平衡这两个有重要的机遇，对行业发展也具有重大的价值。\r\n\r\n我们希望整个行业能够把握好这样一个机遇，正好推动中国经济数据的发展。\r\n\r\n谢谢大家！\r\n\r\n作者简介：周辉 ，环球智财数据科技研究院执行院长、新治理智库联盟发起人、国家行政学院博士后。\r\n\r\n（注：本文根据周辉在4月20日UBDC全域大数据峰会发言整理）\r\n\r\n来源：阿里研究院\r\n\r\n除非注明来源，本站文章均为原创或编译，转载请注明出处并保留链接。数据分析网 » 数据立法：你的隐私谁来保护？\r\n\r\n波斯 编辑\r\n数据分析网编辑，文章均来自网络转载文章或未署名投稿，如您认为本文侵犯了您的版权信息或作者、出处备注有误，请与我们联系修正。联系邮箱afenxi@afenxi.com\r\n\r\n分享到：更多\r\n标签：大数据大数据隐私\r\n为你推荐\r\n车品觉分享：那些年，我在阿里决战大数据\r\nLinkedIn高级分析师王益：大数据时代的理想主义和现实主义\r\n傅一平：数据说谎的艺术\r\n宣晓华：华院大数据商业实践与应用 智能引擎的核心能力\r\n鲍忠铁：五步实现金融行业数据应用\r\n\r\n评论 抢沙发\r\n\r\n\r\n提交评论\r\n昵称 (必填)\r\n邮箱 (必填)\r\n网址\r\n关注我们\r\n微信公众号：数据分析精选\r\n数据分析网旗下微信公众号\r\n大数据交流QQ群\r\n有数据的地方，就有江湖！！\r\n\r\n数据分析网旗下大数据交流QQ群，请根据自己的需要加1-2个群，多了送“飞机票”。\r\n热门文章\r\n博客推荐系统第一部分：物料准备-数据分析网\r\n博客推荐系统第一部分：物料准备\r\n2016-05-14评论()\r\n百度张琪：大数据时代的数据仓储-数据分析网\r\n百度张琪：大数据时代的数据仓储\r\n2016-05-14评论()\r\n一篇文章搞懂R语言回归-数据分析网\r\n一篇文章搞懂R语言回归\r\n2016-06-03评论()\r\n数据产品的前世今生-数据分析网\r\n数据产品的前世今生\r\n2015-11-19评论()\r\n你写的竞品分析，犯了这 5 个错误了吗？-数据分析网\r\n你写的竞品分析，犯了这 5 个错误了吗？\r\n2016-02-22评论()\r\n热门主题\r\n数据分析师 (115)数据分析观点 (5)数据分析资料 (8)小蚊子乐园 (5)数据分析图书 (5)数据信息图 (16)百度 (17)朱冠胤 (2)统计学 (100)大数据 (467)数据可视化 (130)数据管理 (12)数据分析精选 (3)大数据新闻 (113)EMC (1)SAS (30)Excel (29)数据挖掘 (235)数据科学家 (118)数据观点 (125)机器学习 (204)傅志华 (15)电商数据分析 (20)数据分析体系 (24)EverString (4)张溪梦 (21)人工智能 (69)阿里巴巴 (38)Datalab (1)数据分析 (334)\r\n© 2016 数据分析网   All Rights Reserved.\r\n\r\n关于我们 | 加入志愿者 | 免责声明 | 投稿须知 | RSS订阅 | 网站地图\r\n\r\n数据分析网 版权所有 浙ICP备11037353号', '2016-06-05'),
(15, '业务至上', '数据分析网\r\n大数据 \r\n资讯+观点+技术研究中心\r\n首页\r\n大数据新闻\r\n人物观点\r\n大数据\r\n数据分析\r\n软件工具\r\n活动沙龙\r\n投稿\r\n\r\n一个优秀数据分析师具备的11个特性\r\n2016-05-26 来源：数据分析网	分类：数据分析 评论(0) \r\nimg-da11tz\r\n数据分析是一个很复杂的过程，当你成为一名数据分析师，你的身上不知不觉就有了以下这些特征，让我们一起来看看是不是这样：\r\n1、业务至上\r\n\r\n不会把什么方法、什么工具挂在嘴边，首先想到的是你的业务模式是什么？你想解决什么业务问题？\r\n2、用数据说话\r\n\r\n觉得、以为、估计，大概、可能、也许这些词说的越来越少，业务好不好、产品好不好、活动好不好，用数据说话！！\r\n3、对数据负责\r\n\r\n开发TMD又搞错了，产品里面点又漏了…这些都不能当作理由了，自己的数据自己负责，会经常全面、多纬度的校验自己的数据。\r\n4、备注要清楚\r\n\r\n数据来源是什么，数据的定义是什么，数据的时间范围是什么，喜欢开始各种备注，会站在看报告的人角度，把看报告的人当小白。\r\n5、保持好奇心\r\n\r\n好奇心，与生俱来，但时间长、资历老了，很容易被磨灭，我知道、我以为我知道，其实不一定；保持一颗好奇心，扩宽自己的视野，刷新自己的技能，不断拓展你的边界。\r\n6、有备而来\r\n\r\n在正式开讲、分享自己的分析报告之前要先做个彻底的熟悉，逻辑、数据、结论和讲的方式来回梳理个几次，心理面默默的Review几次，不会初当初生牛犊不怕虎、一问问题就懵逼不知道回去查一查分析分析。。。不打无准备的仗。\r\n7、 渴望分享\r\n\r\n熬夜啃书、闷头项目的方式一去不复返，不再自闭门造车，非常十分的渴望把自己的成果分享给大家，与人交流、拓展自己的思维。\r\n8、不可攻击他人\r\n\r\n如1所言，越来越重视业务、越来越看轻技术，不会因为别人使用的是SQL、Excel这些工具就随意攻击他们，因为解决问题才是王道，有时候可能越简单的方式越是有效的，怀着一种平和的心态，海纳百川，有容乃大。\r\n9、平衡生活工作\r\n\r\n数据分析是一种高强度的脑力活动，有时候你的大脑真的非常需要稍作休息，不要盲目不管不顾自己的身体给业务取数、给老板写报告，“取数狂、SQL狂、Excel达人”绝对是不可取的工作方式。每周工作80小时，听上去很牛X，但是你的工作效率肯定要打一个问号，甚至一不小心会犯个不可饶恕的小错误然后前功尽弃。\r\n10、中午打个盹\r\n\r\n1天24小时，不可能全都耗在数据上，这样结果也不会好，停下来，中午打个盹，脑子灵光灵机移动，分析思路在我脑子里。\r\n11、广泛浏览学习\r\n\r\n每天早上会广泛浏览网站，除了虎嗅、36氪之类的，逛逛数据分析网看看大数据世界有什么新鲜事发生、有什么新技术发展、有哪位大咖的观点分享，保持一颗学习的心。\r\n除非注明来源，本站文章均为原创或编译，转载请注明出处并保留链接。数据分析网 » 一个优秀数据分析师具备的11个特性\r\n\r\n数据分析精选 数据分析精选 管理员\r\n数据分析网（www.afenxi.com），让大数据更简单！！文章均来自网络转载文章，我们会把大数据相关的优秀文章转载推荐给大家，如果您认为本文侵犯了您的版权信息或作者、出处备注有误，请与我们联系修正。联系邮箱afenxi@afenxi.com\r\n\r\n分享到：更多\r\n标签：大数据数据分析数据观点\r\n为你推荐\r\n【实操】资深分析师教你竞品分析\r\n产品经理如何通过数据分析来发现用户的“怒点”？\r\n互联网征信中的信用评分模型\r\n天气驱动行业销售大数据\r\n如何让你的数据得到业务方认可？\r\n\r\n评论 抢沙发\r\n\r\n\r\n提交评论\r\n昵称 (必填)\r\n邮箱 (必填)\r\n网址\r\n关注我们\r\n微信公众号：数据分析精选\r\n数据分析网旗下微信公众号\r\n大数据交流QQ群\r\n有数据的地方，就有江湖！！\r\n\r\n数据分析网旗下大数据交流QQ群，请根据自己的需要加1-2个群，多了送“飞机票”。\r\n热门文章\r\nGrowingIO产品实战案例：数据如何驱动产品实践？-数据分析网\r\nGrowingIO产品实战案例：数据如何驱动产品实践？\r\n2016-03-18评论()\r\n数据告诉你：2016哪些科技领域要窜天？-数据分析网\r\n数据告诉你：2016哪些科技领域要窜天？\r\n2016-01-08评论()\r\n租房数据分析：2016年在北京如何租到好房子?-数据分析网\r\n租房数据分析：2016年在北京如何租到好房子?\r\n2016-02-21评论()\r\n周涛：企业如何布局大数据？-数据分析网\r\n周涛：企业如何布局大数据？\r\n2015-10-28评论()\r\n数据分析师的自我修养-数据分析网\r\n数据分析师的自我修养\r\n2016-05-23评论()\r\n热门主题\r\n数据分析师 (115)数据分析观点 (5)数据分析资料 (8)小蚊子乐园 (5)数据分析图书 (5)数据信息图 (16)百度 (17)朱冠胤 (2)统计学 (100)大数据 (467)数据可视化 (130)数据管理 (12)数据分析精选 (3)大数据新闻 (113)EMC (1)SAS (30)Excel (29)数据挖掘 (235)数据科学家 (118)数据观点 (125)机器学习 (204)傅志华 (15)电商数据分析 (20)数据分析体系 (24)EverString (4)张溪梦 (21)人工智能 (69)阿里巴巴 (38)Datalab (1)数据分析 (334)\r\n© 2016 数据分析网   All Rights Reserved.\r\n\r\n关于我们 | 加入志愿者 | 免责声明 | 投稿须知 | RSS订阅 | 网站地图\r\n\r\n数据分析网 版权所有 浙ICP备11037353号', '2016-06-05'),
(16, '金融大数据信用评分模型解析', '数据分析网\r\n大数据 \r\n资讯+观点+技术研究中心\r\n首页\r\n大数据新闻\r\n人物观点\r\n大数据\r\n数据分析\r\n软件工具\r\n活动沙龙\r\n投稿\r\n大数据导航\r\n大数据资料\r\n数据分析视频\r\n数据分析图书\r\n热门主题\r\n作者列表\r\n关于我们\r\n关注本站 \r\nHi, 请登录     我要注册     找回密码\r\n\r\n金融大数据信用评分模型解析\r\n2016-06-02 来源：招商证券	分类：大数据技术 / 数据挖掘 评论(0) \r\n大数据征信\r\n\r\n传统个人征信的分析维度包括：\r\n\r\n1 ）个人基本数据，如年龄、性别、职业、收入、婚姻状况、工作年限、 工作状况等；\r\n\r\n2） 信贷情况，主要是信贷和信用卡相关数据；\r\n\r\n3）公共数据，包括税务、工商、法院、电信、水电煤气等部门的数据；\r\n\r\n4） 个人信用报告查询记录。\r\n\r\n如今随着大数据时代的到来和发展，可用于评估人们的数据越来越丰富，如电商的交易数据、社交类数据（强社交关系如何转化为信用资产）、网络行为数据等， 来自互联网的数据将帮助金融机构更充分地了解客户。\r\n\r\n大数据征信\r\n\r\n（一） 侧重电商： 芝麻信用\r\n\r\n以芝麻信用所构建的信用体系来看，芝麻信用分根据当前采集的个人用户信息进行加工、整理、计算后得出的信用评分，分值范围是 350 到 950，分值越高代表信用水平越好，较高的芝麻分可以帮助个人获得更高效、更优质的服务。 芝麻分综合考虑了个人用户的信用历史、行为偏好、履约能力、身份特质、人脉关系五个维度的信息，其中来自淘宝、支付宝等“阿里系”的数据占 30-40%。\r\n\r\n1） 信用历史： 过往信用账户还款记录及信用账户历史。目前这一块内容大多来自支付宝，特别是支付宝转账和用支付宝还信用卡的历史。\r\n\r\n2） 行为偏好： 在购物、缴费、转账、理财等活动中的偏好及稳定性。比如一个人每天打游戏 10 小时，那么就会被认为是无所事事；如果一个人经常买纸尿裤，那这个人便被认为已为人父母，相对更有责任心。\r\n\r\n3） 履约能力： 包括享用各类信用服务并确保及时履约，例如租车是否按时归还，水电煤气是否按时交费等。\r\n\r\n4） 身份特质： 在使用相关服务过程中留下的足够丰富和可靠的个人基本信息。 包括从公安、学历学籍、工商、法院等公共部门获得的个人资料，未来甚至可能包括根据开车习惯、敲击键盘速度等推测出的个人性格。\r\n\r\n5） 人脉关系： 好友的身份特征以及跟好友互动的程度。根据“物以类聚人以群分”的理论，通过转账关系、校友关系等作为评判个人信用的依据之一。其采用的人脉关系、性格特征等新型变量能否客观反映个人信用，但目前还没有将社交聊天内容、点赞等纳入参考。\r\n\r\n大数据征信大数据征信\r\n\r\n（二） 侧重社交： 腾讯信用\r\n\r\n腾讯信用 主要是基于社交网络。 通过 QQ、微信、财付通、 QQ 空间、腾讯网、 QQ 邮箱等社交网络上的大量信息， 比如在线时长、登录行为、虚拟财产、支付频率、购物习惯、社交行为等， 利用其大数据平台 TDBank，在不同数据源中， 采集并处理包括即时通信、 SNS、电商交易、虚拟消费、关系链、游戏行为、媒体行为和基础画像等数据，并利用统计学、传统机器学习的方法，得出用户信用得分，为用户建立基于互联网信息的个人征信报告。\r\n\r\n腾讯信用 评分以星级的方式展现。 信用星级一共 7 颗星，亮星颗数越多代表信用越良好，星级主要由四个维度构成：\r\n\r\n1） 消费： 用户在微信、手机 QQ 支付以及消费偏好。\r\n\r\n2） 财富： 在腾讯产品内各资产的构成、理财记录。\r\n\r\n3） 安全： 财付通账户是否实名认证和数字认证。\r\n\r\n4） 守约： 消费贷款、信用卡、房贷是否按时还等。\r\n\r\n大数据征信\r\n\r\n（三）侧重运营商： 聚信立\r\n\r\n聚信立主要是基于互联网大数据，综合个人用户运营商数据、电商数据、公积金社保数据、学信网数据等，形成个人信用报告。 聚信立通过借款人授权，利用网页极速抓取技术获取各类用户个人数据，通过海量数据比对和分析，交叉验证，最终为金融机构提供用户的风险分析判断。\r\n\r\n聚信立以报告形式展现，报告主要由四个维度构成：\r\n\r\n1） 信息验真： 通过交叉比对验证用户是否是真实存在的人，是否有欺诈风险。\r\n\r\n2） 运营商数据分析：分析用户生活、工作及社交范围，与家人朋友的联系频率等。\r\n\r\n3） 电商数据分析： 分析用户消费能力及消费习惯，判断用户是否有能力还款。\r\n\r\n4） 其他数据分析： 包括公积金社保数据、学信网数据、全国高法执行名单、黑名单等数据，判断用户是否存在欺诈风险。\r\n\r\n聚信立的底层 IT架构为丰富的技术线提供稳定支持，对所有数据源网站进行实时监控，人工智能自动排错，可用率超过 90%。\r\n\r\n大数据征信\r\n\r\n（四）侧重信用卡： 51 信用卡\r\n\r\n51 信用卡主要是基于用户信用卡电子账单历史分析、电商及社交关系强交叉验证。 根据用户的信用卡数据、开放给平台的电商数据所对应的购买行为、手机运营商的通话情况、登记信息等取得多维信息的交叉验证，确定用户的风险等级以及是否贷款给该用户。\r\n\r\n51 信用卡风险等级由五个维度构成：\r\n\r\n1） 账单管理时间： 信用卡有效存续时间越长，用户风险越低。\r\n\r\n2） 账单表现： 根据用户的授信卡数、授信额度，以及还款比和账单完整度判断用户的还款能力和诚信程度。\r\n\r\n3） 手机入网期限： 手机入网期限越长，用户风险越低。\r\n\r\n4） 运营商： 通过近 4 个月有效通话记录以及通讯录中是否存在负面联系人判断用户自身的可靠程度。\r\n\r\n5） 淘宝： 主要看常用收货姓名及电话号码是否与申请人预留号码一致。\r\n\r\n大数据征信\r\n\r\n大数据征信怎么做？随着大数据时代的到来和发展，可用于评估人们的数据越来越丰富，如电商的交易数据、社交类数据（强社交关系如何转化为信用资产）、网络行为数据等，来自互联网的数据将帮助金融机构更充分地了解客户。\r\n\r\n1）侧重电商：芝麻信用。芝麻分来自淘宝、支付宝的数据占30-40%，综合考虑个人用户的信用历史、行为偏好、履约能力、身份特质、人脉关系五个维度的信息；\r\n\r\n2）侧重社交：腾讯信用。通过社交网络上的大量信息，比如在线时长、登录行为、虚拟财产、支付频率、购物习惯、社交行为等，得出用户信用得分；\r\n\r\n3）侧重运营商：聚信立。综合个人用户运营商数据、电商数据、公积金社保数据、学信网数据等，形成个人信用报告；\r\n\r\n4）侧重信用卡：51信用卡。根据用户的信用卡数据、开放给平台的电商数据所对应的购买行为、手机运营商的通话情况、登记信息等取得多维信息的交叉验证，确定用户风险等级。\r\n\r\n节选自招商证券2016年5月17日发布的报告《全民征信时代开启，大数据推动创新》。\r\n\r\n无觅相关文章插件，快速提升流量\r\n除非注明来源，本站文章均为原创或编译，转载请注明出处并保留链接。数据分析网 » 金融大数据信用评分模型解析\r\n\r\ndaxi daxi 编辑\r\nKeep It Simple, Stupid!\r\n\r\n分享到：更多\r\n标签：信用评分模型大数据数据挖掘模型\r\n为你推荐\r\n使用混淆矩阵(Confusion matrix)对分类模型进行评估\r\n运行于云端的Hadoop——数据即服务的论证\r\n大数据分析界的“神兽”Apache Kylin有多牛？\r\n码农的良心推荐：9个最佳的大数据处理编程语言\r\n解密Uber数据科学团队路径选择算法的优化之路\r\n\r\n评论 抢沙发\r\n\r\n\r\n提交评论\r\n昵称 (必填)\r\n邮箱 (必填)\r\n网址\r\n关注我们\r\n微信公众号：数据分析精选\r\n数据分析网旗下微信公众号\r\n大数据交流QQ群\r\n有数据的地方，就有江湖！！\r\n\r\n数据分析网旗下大数据交流QQ群，请根据自己的需要加1-2个群，多了送“飞机票”。\r\n热门文章\r\n数据分析入门8：数据分析入门图书推荐-数据分析网\r\n数据分析入门8：数据分析入门图书推荐\r\n2015-11-08评论()\r\n你用Python做过什么有趣的数据挖掘项目？-数据分析网\r\n你用Python做过什么有趣的数据挖掘项目？\r\n2016-01-26评论()\r\n如何让数据像高圆圆一样美？-数据分析网\r\n如何让数据像高圆圆一样美？\r\n2016-01-11评论()\r\n【收藏】DT时代，你需要知道的数据报告网站-数据分析网\r\n【收藏】DT时代，你需要知道的数据报告网站\r\n2016-05-15评论()\r\n5种方法教创业公司如何利用大数据形成独特竞争优势-数据分析网\r\n5种方法教创业公司如何利用大数据形成独特竞争优势\r\n2015-11-13评论()\r\n热门主题\r\n数据分析师 (115)数据分析观点 (5)数据分析资料 (8)小蚊子乐园 (5)数据分析图书 (5)数据信息图 (16)百度 (17)朱冠胤 (2)统计学 (100)大数据 (467)数据可视化 (130)数据管理 (12)数据分析精选 (3)大数据新闻 (113)EMC (1)SAS (30)Excel (29)数据挖掘 (235)数据科学家 (118)数据观点 (125)机器学习 (204)傅志华 (15)电商数据分析 (20)数据分析体系 (24)EverString (4)张溪梦 (21)人工智能 (69)阿里巴巴 (38)Datalab (1)数据分析 (334)\r\n© 2016 数据分析网   All Rights Reserved.\r\n\r\n关于我们 | 加入志愿者 | 免责声明 | 投稿须知 | RSS订阅 | 网站地图\r\n\r\n数据分析网 版权所有 浙ICP备11037353号', '2016-06-05');

DROP TABLE IF EXISTS `payroll_payroll`;
CREATE TABLE IF NOT EXISTS `payroll_payroll` (
  `id` int(11) NOT NULL,
  `year` int(11) NOT NULL,
  `month` int(11) NOT NULL,
  `hour` int(11) NOT NULL,
  `salary` double NOT NULL,
  `employee_id` int(11) NOT NULL,
  `employeeType_id` int(11) NOT NULL
) ENGINE=InnoDB AUTO_INCREMENT=13 DEFAULT CHARSET=utf8;

INSERT INTO `payroll_payroll` (`id`, `year`, `month`, `hour`, `salary`, `employee_id`, `employeeType_id`) VALUES
(1, 2016, 4, 242, 4840, 1, 1),
(2, 2016, 4, 239, 23900, 2, 2),
(3, 2016, 5, 249, 4980, 1, 1),
(4, 2016, 5, 243, 24300, 2, 2),
(5, 2016, 5, 273, 8190, 3, 3),
(6, 2016, 5, 286, 20020, 4, 4),
(7, 2016, 5, 263, 36820, 5, 5),
(8, 2016, 5, 278, 19460, 6, 4),
(9, 2016, 5, 268, 26800, 7, 2),
(10, 2016, 5, 270, 8100, 8, 3),
(11, 2016, 5, 273, 19110, 9, 4),
(12, 2016, 5, 266, 7980, 10, 3);

DROP TABLE IF EXISTS `payroll_post`;
CREATE TABLE IF NOT EXISTS `payroll_post` (
  `id` int(11) NOT NULL,
  `title` varchar(255) NOT NULL,
  `content` longtext NOT NULL,
  `pubdate` date NOT NULL,
  `modifyDateTime` datetime(6) NOT NULL,
  `employee_id` int(11) NOT NULL
) ENGINE=InnoDB AUTO_INCREMENT=20 DEFAULT CHARSET=utf8;

INSERT INTO `payroll_post` (`id`, `title`, `content`, `pubdate`, `modifyDateTime`, `employee_id`) VALUES
(1, 'OLDboy-python第八期运维教程', '从0基础开始讲解Python语法、流程控制、函数式编程、面向对象开发等最重要的PYTHON开发必备基础知识,百度搜索：小白资源网', '2016-06-05', '2016-06-05 07:19:20.338205', 1),
(2, '我喜欢一个人独处时，吃饺子，因为，，，', '记得小时候家，到了中学以后，每到中午放学，就一个人，去饭店要半斤饺子，一个人一边吃一边品，品味酸甜苦辣，品味家的味道，现在应喜欢一个人，在心烦意乱的时候，静下来，一个人，吃了又停，停了又吃，，，，，', '2016-06-05', '2016-06-05 07:22:44.886429', 1),
(3, '管不住嘴、迈不开腿，可我还是瘦了呀', '忠告：管不住嘴巴的人，总有一天会得肠胃炎的。\r\n\r\n饮食调整到正常人的摄入量之后（零食照吃），我的BMI指数很快就降到了19以下。\r\n\r\nBMI减到19之后，再要继续降，需要开始辅助一些轻度运动，拉伸、广播体操即可，每天10分钟，半个月就足够帮助你BMI指数从19减到18。\r\n\r\nBMI指数18-19，身材评分已经基本可以达到80分，意味着你穿大部分日常的衣服都能得到“看起来不错”的效果了。当然，想要实现“好漂亮”的穿衣效果，需要身材分数达到90分以上，BMI指数更低。\r\n\r\nBMI指数到18以下，就必须以运动塑形为主了。在这个区间内，同样的身高和体重，有的人身材健美，有的人却皮包骨头形同骷髅。相信那种骷髅身材绝不是你想要的。此时，你的体重已经不重要，防止变成骷髅身材，才是重中之重。\r\n\r\n到了这个阶段，后续的路会需要付出很多，不再像之前那样只改一下饮食就能实现。因为从不及格到良好，只需要改掉坏习惯；而从良好到优秀，需要大量的好习惯。\r\n\r\n我目前还没达到目标，等到实现之后，再分享“BMI指数减到17以下，身材达到90分以上”的方法吧。\r\n\r\n毕竟，这样轻松地减到BMI指数18-19，你穿些日常的漂亮衣服已经没问题了。\r\n\r\n在不运动、不节食、不吃减肥药的情况下，我从BMI指数23、身材评分不足60分，实现BMI指数18、身材评分86分，用了4个月，没有什么刻意，调整了饮食之后自然而然地就降下来。（别忘了在此期间我依然吃的比正常人多）\r\n\r\n只要合理饮食、不超标，相信你拥有好身材会比我快。\r\n\r\n“人生苦短，必须性感。”\r\n\r\n— End —\r\n\r\n文／诸葛靓（简书作者）\r\n原文链接：http://www.jianshu.com/p/542071c245bb\r\n著作权归作者所有，转载请联系作者获得授权，并标注“简书作者”。', '2016-06-05', '2016-06-05 07:22:35.637625', 2),
(4, 'JavaScript在浏览器中使用事项', '在HTML里嵌入JavaScript\r\n在HTML文档里嵌入客户端JavaScript代码有4种方法：\r\n\r\n内联，放置在<script>和</script>标签对之间。\r\n放置在<script>标签的src属性指定的外部文件中。\r\n放置在HTML事件处理程序中，由HTML属性onclick或onmousemove等指定。\r\n放在一个URL里，这个URL使用特殊的"javascript:"协议。\r\n有个编程哲学叫"unobtrusive JavaScript"，主张内容(HTML)和行为(JavaScript代码)应该尽量地保持分离。根据这个编程哲学，JavaScript最好通过<script>元素的src属性来嵌入HTML文档里。\r\n\r\n文／kissLife（简书作者）\r\n原文链接：http://www.jianshu.com/p/c7319e628916\r\n著作权归作者所有，转载请联系作者获得授权，并标注“简书作者”。', '2016-06-05', '2016-06-05 07:21:50.597183', 2),
(5, 'Git教程笔记（六·自定义Git）', 'git add -f <filename> #强行添加文件\r\n\r\ngit check-ignore #检查.gitignore规则\r\n\r\ngit config --global alias.<> <> #配置别名', '2016-06-06', '2016-06-06 10:04:27.802673', 3),
(6, '标题就开始会计法', '使用Windows的童鞋注意了，如果你在资源管理器里新建一个.gitignore文件，它会非常弱智地提示你必须输入文件名，但是在文本编辑器里“保存”或者“另存为”就可以把文件保存为.gitignore了。', '2016-06-05', '2016-06-05 07:21:16.643193', 4),
(7, 'C语言的变量作用域及修饰符', '[a]、局部变量：\r\n【定义】:\r\n在大括号以内的变量叫做局部变量 （包括形参），大括号{}以内的,大括号{}并不会影响执行流程。函数的大括号{}也是局部变量。\r\n其作用域（作用域->起到作用的区域或者范围叫做作用域）是从定义的位置到大括号结束，而且局部变量的值没有初始化值，而是随机的。\r\n\r\n文／我是蔡金龙（简书作者）\r\n原文链接：http://www.jianshu.com/p/74f2a0d6595b\r\n著作权归作者所有，转载请联系作者获得授权，并标注“简书作者”。', '2016-06-05', '2016-06-05 07:17:06.868869', 5),
(8, '也许你永远不能进化--程序猿的形与神', '河上有一艘船，叫做知识。\r\n\r\n上这艘船很多办法，其中一项就是读书。很多人以为，买了书，读完了，就上了船。\r\n\r\n上了船，自然这艘船就会达到河对面，到达真理之岸。', '2016-06-05', '2016-06-05 07:17:16.435756', 5),
(9, '考虑用静态工厂方法代替构造器', '阅读经典——《Effective Java》01\r\n我们通常使用构造器来创建对象。除此之外，本文介绍另一种创建对象的方式——使用静态工厂方法。', '2016-06-05', '2016-06-05 07:21:21.901433', 6),
(10, '10款最受用户欢迎的图形图表制作工具', '在数字经济时代，人们需要对大量的数字进行分析，帮助用户更直观的察觉差异，做出判断，减少时间成本。当然，你可能想象不到这种数据可视化的技术可以追溯到2500年前世界上的第一张地图，但是，如今利用各种形态的数据可视化图表帮助用户减少分析时间，快速做出决策一直扮演着重要的角色。﻿\r\n\r\n。', '2016-06-05', '2016-06-05 07:21:27.356003', 6),
(11, 'jQuery简明参考手册——30分钟快速入门jQuery', '1、基本选择器\r\n$("#id")：id选择器，返回单个元素\r\n$(".class")：class选择器，返回集合元素\r\n$("element")：选定指定的元素名匹配的元素，返回集合元素\r\n$("*")：通配符选择器，选择所有元素，返回集合元素\r\n$("selector1,selector2")：选择所有选择器匹配的元素，返回集合元素\r\n\r\n文／忽如寄（简书作者）\r\n原文链接：http://www.jianshu.com/p/3e2768c8dad4\r\n著作权归作者所有，转载请联系作者获得授权，并标注“简书作者”。', '2016-06-05', '2016-06-05 07:21:32.762664', 6),
(12, 'Hexo 主题设置', 'Hexo 支持个性化定制主题，可以根据自己的喜好进行修改，想获取更多主题点击这里哦。', '2016-06-05', '2016-06-05 07:18:31.451161', 7),
(13, 'jQuery选择器', '不会用选择器，望元素兴叹，难受的要死！在此全面的记录一遍。看完就彻底知之了！\r\njquery的选择器分为以下几类\r\n\r\n基本选择器\r\n层次选择器\r\n过滤选择器\r\n表单选择器', '2016-06-05', '2016-06-05 07:18:53.895489', 7),
(14, 'Cocoapods私有仓库封装过程中的思考', '背景：随着公司相关APP项目的开展，公用框架的创建与维护越发显得迫切起来。因为工作中经常接触使用cocoapods,也知道她其实可以搞定这件事，所以就首当其冲的选择了基于cocoapods的封装方案。', '2016-06-05', '2016-06-05 07:19:14.920265', 7),
(15, '如何写好函数', '函数的第一规则是短小', '2016-06-05', '2016-06-05 07:21:10.551003', 3),
(16, '3D max 常用快捷键', '在作图的时候，不知道大家是否常用快捷键呢？下面是整理的是一些比较常用的3D max快捷键，希望大家都能派上用场。', '2016-06-05', '2016-06-05 07:22:22.640908', 3),
(19, '没有bug', 'no bug', '2016-06-06', '2016-06-06 10:04:18.282856', 1);


ALTER TABLE `auth_group`
  ADD PRIMARY KEY (`id`), ADD UNIQUE KEY `name` (`name`);

ALTER TABLE `auth_group_permissions`
  ADD PRIMARY KEY (`id`), ADD UNIQUE KEY `auth_group_permissions_group_id_0cd325b0_uniq` (`group_id`,`permission_id`), ADD KEY `auth_group_permissi_permission_id_84c5c92e_fk_auth_permission_id` (`permission_id`);

ALTER TABLE `auth_permission`
  ADD PRIMARY KEY (`id`), ADD UNIQUE KEY `auth_permission_content_type_id_01ab375a_uniq` (`content_type_id`,`codename`);

ALTER TABLE `auth_user`
  ADD PRIMARY KEY (`id`), ADD UNIQUE KEY `username` (`username`);

ALTER TABLE `auth_user_groups`
  ADD PRIMARY KEY (`id`), ADD UNIQUE KEY `auth_user_groups_user_id_94350c0c_uniq` (`user_id`,`group_id`), ADD KEY `auth_user_groups_group_id_97559544_fk_auth_group_id` (`group_id`);

ALTER TABLE `auth_user_user_permissions`
  ADD PRIMARY KEY (`id`), ADD UNIQUE KEY `auth_user_user_permissions_user_id_14a6b632_uniq` (`user_id`,`permission_id`), ADD KEY `auth_user_user_perm_permission_id_1fbb5f2c_fk_auth_permission_id` (`permission_id`);

ALTER TABLE `django_admin_log`
  ADD PRIMARY KEY (`id`), ADD KEY `django_admin__content_type_id_c4bce8eb_fk_django_content_type_id` (`content_type_id`), ADD KEY `django_admin_log_user_id_c564eba6_fk_auth_user_id` (`user_id`);

ALTER TABLE `django_content_type`
  ADD PRIMARY KEY (`id`), ADD UNIQUE KEY `django_content_type_app_label_76bd3d3b_uniq` (`app_label`,`model`);

ALTER TABLE `django_migrations`
  ADD PRIMARY KEY (`id`);

ALTER TABLE `django_session`
  ADD PRIMARY KEY (`session_key`), ADD KEY `django_session_de54fa62` (`expire_date`);

ALTER TABLE `payroll_attendrecord`
  ADD PRIMARY KEY (`id`), ADD KEY `payroll_attendrecord_dcc97e32` (`employee_id`);

ALTER TABLE `payroll_comment`
  ADD PRIMARY KEY (`id`), ADD KEY `payroll_comment_dcc97e32` (`employee_id`), ADD KEY `payroll_comment_f3aa1999` (`post_id`);

ALTER TABLE `payroll_department`
  ADD PRIMARY KEY (`id`), ADD UNIQUE KEY `name` (`name`);

ALTER TABLE `payroll_employee`
  ADD PRIMARY KEY (`id`), ADD UNIQUE KEY `user_id` (`user_id`), ADD UNIQUE KEY `employeeId` (`employeeId`), ADD KEY `payroll_employee_department_id_59b5089e_fk_payroll_department_id` (`department_id`), ADD KEY `payroll_employee_94757cae` (`type_id`);

ALTER TABLE `payroll_employeetype`
  ADD PRIMARY KEY (`id`), ADD UNIQUE KEY `name` (`name`);

ALTER TABLE `payroll_notice`
  ADD PRIMARY KEY (`id`);

ALTER TABLE `payroll_payroll`
  ADD PRIMARY KEY (`id`), ADD KEY `payroll_payroll_employee_id_cd24ccf6_fk_payroll_employee_id` (`employee_id`), ADD KEY `payroll_payr_employeeType_id_5d42935b_fk_payroll_employeetype_id` (`employeeType_id`);

ALTER TABLE `payroll_post`
  ADD PRIMARY KEY (`id`), ADD KEY `payroll_post_employee_id_ddcf62f4_fk_payroll_employee_id` (`employee_id`);


ALTER TABLE `auth_group`
  MODIFY `id` int(11) NOT NULL AUTO_INCREMENT;
ALTER TABLE `auth_group_permissions`
  MODIFY `id` int(11) NOT NULL AUTO_INCREMENT;
ALTER TABLE `auth_permission`
  MODIFY `id` int(11) NOT NULL AUTO_INCREMENT,AUTO_INCREMENT=43;
ALTER TABLE `auth_user`
  MODIFY `id` int(11) NOT NULL AUTO_INCREMENT,AUTO_INCREMENT=12;
ALTER TABLE `auth_user_groups`
  MODIFY `id` int(11) NOT NULL AUTO_INCREMENT;
ALTER TABLE `auth_user_user_permissions`
  MODIFY `id` int(11) NOT NULL AUTO_INCREMENT;
ALTER TABLE `django_admin_log`
  MODIFY `id` int(11) NOT NULL AUTO_INCREMENT,AUTO_INCREMENT=63;
ALTER TABLE `django_content_type`
  MODIFY `id` int(11) NOT NULL AUTO_INCREMENT,AUTO_INCREMENT=15;
ALTER TABLE `django_migrations`
  MODIFY `id` int(11) NOT NULL AUTO_INCREMENT,AUTO_INCREMENT=17;
ALTER TABLE `payroll_attendrecord`
  MODIFY `id` int(11) NOT NULL AUTO_INCREMENT,AUTO_INCREMENT=456;
ALTER TABLE `payroll_comment`
  MODIFY `id` int(11) NOT NULL AUTO_INCREMENT,AUTO_INCREMENT=41;
ALTER TABLE `payroll_department`
  MODIFY `id` int(11) NOT NULL AUTO_INCREMENT,AUTO_INCREMENT=4;
ALTER TABLE `payroll_employee`
  MODIFY `id` int(11) NOT NULL AUTO_INCREMENT,AUTO_INCREMENT=11;
ALTER TABLE `payroll_employeetype`
  MODIFY `id` int(11) NOT NULL AUTO_INCREMENT,AUTO_INCREMENT=6;
ALTER TABLE `payroll_notice`
  MODIFY `id` int(11) NOT NULL AUTO_INCREMENT,AUTO_INCREMENT=17;
ALTER TABLE `payroll_payroll`
  MODIFY `id` int(11) NOT NULL AUTO_INCREMENT,AUTO_INCREMENT=13;
ALTER TABLE `payroll_post`
  MODIFY `id` int(11) NOT NULL AUTO_INCREMENT,AUTO_INCREMENT=20;

ALTER TABLE `auth_group_permissions`
ADD CONSTRAINT `auth_group_permissi_permission_id_84c5c92e_fk_auth_permission_id` FOREIGN KEY (`permission_id`) REFERENCES `auth_permission` (`id`),
ADD CONSTRAINT `auth_group_permissions_group_id_b120cbf9_fk_auth_group_id` FOREIGN KEY (`group_id`) REFERENCES `auth_group` (`id`);

ALTER TABLE `auth_permission`
ADD CONSTRAINT `auth_permissi_content_type_id_2f476e4b_fk_django_content_type_id` FOREIGN KEY (`content_type_id`) REFERENCES `django_content_type` (`id`);

ALTER TABLE `auth_user_groups`
ADD CONSTRAINT `auth_user_groups_group_id_97559544_fk_auth_group_id` FOREIGN KEY (`group_id`) REFERENCES `auth_group` (`id`),
ADD CONSTRAINT `auth_user_groups_user_id_6a12ed8b_fk_auth_user_id` FOREIGN KEY (`user_id`) REFERENCES `auth_user` (`id`);

ALTER TABLE `auth_user_user_permissions`
ADD CONSTRAINT `auth_user_user_perm_permission_id_1fbb5f2c_fk_auth_permission_id` FOREIGN KEY (`permission_id`) REFERENCES `auth_permission` (`id`),
ADD CONSTRAINT `auth_user_user_permissions_user_id_a95ead1b_fk_auth_user_id` FOREIGN KEY (`user_id`) REFERENCES `auth_user` (`id`);

ALTER TABLE `django_admin_log`
ADD CONSTRAINT `django_admin__content_type_id_c4bce8eb_fk_django_content_type_id` FOREIGN KEY (`content_type_id`) REFERENCES `django_content_type` (`id`),
ADD CONSTRAINT `django_admin_log_user_id_c564eba6_fk_auth_user_id` FOREIGN KEY (`user_id`) REFERENCES `auth_user` (`id`);

ALTER TABLE `payroll_attendrecord`
ADD CONSTRAINT `payroll_attendrecord_employee_id_b5f3cde4_fk_payroll_employee_id` FOREIGN KEY (`employee_id`) REFERENCES `payroll_employee` (`id`);

ALTER TABLE `payroll_comment`
ADD CONSTRAINT `payroll_comment_employee_id_b07bfaf2_fk_payroll_employee_id` FOREIGN KEY (`employee_id`) REFERENCES `payroll_employee` (`id`),
ADD CONSTRAINT `payroll_comment_post_id_1dcfc4ca_fk_payroll_post_id` FOREIGN KEY (`post_id`) REFERENCES `payroll_post` (`id`);

ALTER TABLE `payroll_employee`
ADD CONSTRAINT `payroll_employee_department_id_59b5089e_fk_payroll_department_id` FOREIGN KEY (`department_id`) REFERENCES `payroll_department` (`id`),
ADD CONSTRAINT `payroll_employee_type_id_244cec5e_fk_payroll_employeetype_id` FOREIGN KEY (`type_id`) REFERENCES `payroll_employeetype` (`id`),
ADD CONSTRAINT `payroll_employee_user_id_ea80fd24_fk_auth_user_id` FOREIGN KEY (`user_id`) REFERENCES `auth_user` (`id`);

ALTER TABLE `payroll_payroll`
ADD CONSTRAINT `payroll_payr_employeeType_id_5d42935b_fk_payroll_employeetype_id` FOREIGN KEY (`employeeType_id`) REFERENCES `payroll_employeetype` (`id`),
ADD CONSTRAINT `payroll_payroll_employee_id_cd24ccf6_fk_payroll_employee_id` FOREIGN KEY (`employee_id`) REFERENCES `payroll_employee` (`id`);

ALTER TABLE `payroll_post`
ADD CONSTRAINT `payroll_post_employee_id_ddcf62f4_fk_payroll_employee_id` FOREIGN KEY (`employee_id`) REFERENCES `payroll_employee` (`id`);

DELIMITER $$
DROP EVENT `runGetPayroll`$$
CREATE DEFINER=`root`@`localhost` EVENT `runGetPayroll` ON SCHEDULE EVERY 1 MONTH STARTS '2016-06-30 23:00:00' ENDS '2016-06-30 23:30:00' ON COMPLETION PRESERVE ENABLE DO call getPayroll(2016,5)$$

DELIMITER ;

/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
